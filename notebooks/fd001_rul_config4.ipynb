{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FD001 RUL Prediction \u2014 Config 4: Per-Cycle + RT Geometry + ORTHON\n",
    "\n",
    "**Target benchmark:** 11.72 RMSE (the best prior result)\n",
    "\n",
    "**Architecture:**\n",
    "- Layer 1: Cycle number (1)\n",
    "- Layer 2: Raw sensors \u2014 15 varying (15)\n",
    "- Layer 3: Rolling stats \u2014 15 sensors \u00d7 5 windows \u00d7 5 stats (375)\n",
    "- Layer 4: Delta features \u2014 sensor[t] - sensor[t-1] (15)\n",
    "- Layer 5: RT geometry \u2014 fleet healthy baseline, per cycle (5)\n",
    "- Layer 6: ORTHON features \u2014 eigendecomp derivatives + homology + trajectory, asof-joined from ml/ (window \u2192 cycle)\n",
    "\n",
    "**Models:**\n",
    "- Run A: Ridge / RandomForest / GradientBoosting (comparable to v2)\n",
    "- Run B: LGB + XGB + HistGBM \u2192 RidgeCV stacking (matches 11.72 architecture)\n",
    "\n",
    "**Key constraint:** Fleet baseline fitted on TRAIN only, applied identically to TRAIN and TEST.\n",
    "All features are strictly backward-looking (no future lookahead)."
   ],
   "id": "ac527091"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "31072682"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TRAIN_BASE = Path('/Users/jasonrudder/domains/cmapss/FD_001/Train')\n",
    "TEST_BASE  = Path('/Users/jasonrudder/domains/cmapss/FD_001/Test')\n",
    "TRAIN_ML   = TRAIN_BASE / 'output_time/ml'\n",
    "TEST_ML    = TEST_BASE  / 'output_time/ml'\n",
    "RUL_PATH   = Path('/Users/jasonrudder/domains/cmapss/FD_001/RUL_FD001.txt')\n",
    "\n",
    "# The 15 sensors used in the 11.72 benchmark (excluding 7 constants + op1/op2 operational settings)\n",
    "SENSORS_15 = ['BPR', 'NRc', 'NRf', 'Nc', 'Nf', 'P15', 'P30', 'Ps30',\n",
    "               'T24', 'T30', 'T50', 'W31', 'W32', 'htBleed', 'phi']\n",
    "\n",
    "ROLL_WINDOWS  = [5, 10, 15, 20, 30]\n",
    "EARLY_LIFE    = 30     # first N cycles define fleet healthy baseline\n",
    "N_PCA         = 10     # PCA components for fleet baseline\n",
    "RUL_CAP       = 125\n",
    "N_FOLDS       = 5\n",
    "SEED          = 42\n",
    "\n",
    "print('Config ready.')\n",
    "print(f'  {len(SENSORS_15)} sensors, {len(ROLL_WINDOWS)} roll windows')\n",
    "print(f'  Rolling features: {len(SENSORS_15) * len(ROLL_WINDOWS) * 5} = {len(SENSORS_15)} \u00d7 {len(ROLL_WINDOWS)} \u00d7 5 stats')"
   ],
   "id": "6b92f1d5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Observations \u2192 Wide Format"
   ],
   "id": "562385b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_wide(obs_path: Path, sensors: list) -> pd.DataFrame:\n    \"\"\"Load observations.parquet, filter to sensors, pivot to one row per (cohort, cycle).\"\"\"\n    obs = pl.read_parquet(str(obs_path))\n    obs_s = obs.filter(pl.col('signal_id').is_in(sensors))\n    wide = (\n        obs_s\n        .pivot(index=['cohort', 'signal_0'], on='signal_id', values='value',\n               aggregate_function='first')\n        .sort(['cohort', 'signal_0'])\n    )\n    df = wide.to_pandas().rename(columns={'signal_0': 'cycle'})\n    return df\n\nprint('Loading train...')\ntrain_wide = load_wide(TRAIN_BASE / 'observations.parquet', SENSORS_15)\nprint(f'  Train: {train_wide.shape} \u2014 {train_wide[\"cohort\"].nunique()} engines')\n\nprint('Loading test...')\ntest_wide = load_wide(TEST_BASE / 'observations.parquet', SENSORS_15)\nprint(f'  Test:  {test_wide.shape} \u2014 {test_wide[\"cohort\"].nunique()} engines')",
   "id": "7fb9e474"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RUL Labels"
   ],
   "id": "353c8a97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_train_rul(df: pd.DataFrame, cap: int = 125) -> pd.DataFrame:\n",
    "    \"\"\"RUL = max_cycle_per_engine - current_cycle, capped.\"\"\"\n",
    "    df = df.copy()\n",
    "    max_cycle = df.groupby('cohort')['cycle'].transform('max')\n",
    "    df['RUL'] = np.clip((max_cycle - df['cycle']).values, 0, cap)\n",
    "    return df\n",
    "\n",
    "train_wide = add_train_rul(train_wide, RUL_CAP)\n",
    "\n",
    "# Load test ground truth (ordered by engine number, 1-indexed)\n",
    "rul_gt = np.loadtxt(str(RUL_PATH))\n",
    "print(f'Train RUL: {train_wide[\"RUL\"].min():.0f} \u2013 {train_wide[\"RUL\"].max():.0f} ({len(train_wide)} rows)')\n",
    "print(f'Test GT RUL: {rul_gt.min():.0f} \u2013 {rul_gt.max():.0f} ({len(rul_gt)} engines)')"
   ],
   "id": "5f979685"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Per-Cycle Features (Rolling + Delta)"
   ],
   "id": "ef75b706"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def add_per_cycle_features(df: pd.DataFrame, sensors: list, windows: list) -> pd.DataFrame:\n    \"\"\"Add rolling stats and delta features. Processes per engine to prevent bleed.\"\"\"\n    df = df.sort_values(['cohort', 'cycle']).copy()\n    results = []\n    for cohort_id, grp in df.groupby('cohort', sort=False):\n        grp = grp.reset_index(drop=True)\n        # Rolling stats (5 stats \u00d7 5 windows \u00d7 15 sensors = 375)\n        new_cols = {}\n        for win in windows:\n            r = grp[sensors].rolling(win, min_periods=1)\n            means = r.mean()\n            stds  = r.std(ddof=0).fillna(0.0)\n            mins  = r.min()\n            maxs  = r.max()\n            for s in sensors:\n                new_cols[f'roll_{s}_mean_{win}']  = means[s].values\n                new_cols[f'roll_{s}_std_{win}']   = stds[s].values\n                new_cols[f'roll_{s}_min_{win}']   = mins[s].values\n                new_cols[f'roll_{s}_max_{win}']   = maxs[s].values\n                new_cols[f'roll_{s}_range_{win}'] = (maxs[s] - mins[s]).values\n        for s in sensors:\n            new_cols[f'delta_{s}'] = grp[s].diff(1).fillna(0.0).values\n        grp = pd.concat([grp, pd.DataFrame(new_cols, index=grp.index)], axis=1)\n        results.append(grp)\n    return pd.concat(results, ignore_index=True)\n\nprint('Building train per-cycle features...')\ntrain_feat = add_per_cycle_features(train_wide, SENSORS_15, ROLL_WINDOWS)\nn_per_cycle = 1 + len(SENSORS_15) + len(SENSORS_15)*len(ROLL_WINDOWS)*5 + len(SENSORS_15)  # cycle+sensors+rolling+delta\nprint(f'  Train: {train_feat.shape} \u2014 expect ~{n_per_cycle} feature columns')\n\nprint('Building test per-cycle features...')\ntest_feat = add_per_cycle_features(test_wide, SENSORS_15, ROLL_WINDOWS)\nprint(f'  Test:  {test_feat.shape}')",
   "id": "26834046"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fleet Baseline + RT Geometry\n",
    "\n",
    "Fleet centroid = mean of the first `EARLY_LIFE` cycles pooled across ALL training engines.  \n",
    "Fitted on TRAIN only. Applied identically to train and test (no leakage).\n",
    "\n",
    "5 RT geometry features per cycle:\n",
    "- `rt_centroid_dist` \u2014 L2 distance from fleet mean (in standardized space)\n",
    "- `rt_centroid_dist_norm` \u2014 distance normalized by \u221an_sensors\n",
    "- `rt_pc1_proj` \u2014 projection onto primary degradation axis\n",
    "- `rt_pc2_proj` \u2014 projection onto secondary axis\n",
    "- `rt_mahalanobis` \u2014 eigenvalue-weighted distance"
   ],
   "id": "c4240df8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fleet baseline from early-life train cycles\n",
    "early_data = train_wide[train_wide['cycle'] <= EARLY_LIFE][SENSORS_15].dropna()\n",
    "fleet_scaler = StandardScaler()\n",
    "early_scaled = fleet_scaler.fit_transform(early_data.values)\n",
    "fleet_pca = PCA(n_components=N_PCA, random_state=SEED)\n",
    "fleet_pca.fit(early_scaled)\n",
    "\n",
    "n_early_engines = train_wide[train_wide['cycle'] <= EARLY_LIFE]['cohort'].nunique()\n",
    "print(f'Fleet baseline: {len(early_data):,} early-life cycles from {n_early_engines} engines')\n",
    "print(f'PC1 explains {fleet_pca.explained_variance_ratio_[0]:.1%}, '\n",
    "      f'PC2 {fleet_pca.explained_variance_ratio_[1]:.1%}, '\n",
    "      f'top-5 cumulative {fleet_pca.explained_variance_ratio_[:5].sum():.1%}')"
   ],
   "id": "e32b8ff0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rt_geometry(df: pd.DataFrame, sensors: list,\n",
    "                        scaler: StandardScaler, pca: PCA) -> pd.DataFrame:\n",
    "    \"\"\"Compute 5 RT geometry features per row using the pre-fitted fleet baseline.\"\"\"\n",
    "    X = df[sensors].values.astype(np.float64)\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    # Centroid distance (fleet centroid = origin in scaled space)\n",
    "    centroid_dist      = np.linalg.norm(X_scaled, axis=1)\n",
    "    centroid_dist_norm = centroid_dist / np.sqrt(X_scaled.shape[1])\n",
    "\n",
    "    # PC projections\n",
    "    X_proj    = pca.transform(X_scaled)        # (n, n_pcs)\n",
    "    pc1_proj  = X_proj[:, 0]\n",
    "    pc2_proj  = X_proj[:, 1] if pca.n_components_ > 1 else np.zeros(len(X_scaled))\n",
    "\n",
    "    # Mahalanobis approximation\n",
    "    lambdas   = np.maximum(pca.explained_variance_, 1e-10)\n",
    "    mahal     = np.sqrt(np.sum(X_proj**2 / lambdas, axis=1))\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'rt_centroid_dist':      centroid_dist,\n",
    "        'rt_centroid_dist_norm': centroid_dist_norm,\n",
    "        'rt_pc1_proj':           pc1_proj,\n",
    "        'rt_pc2_proj':           pc2_proj,\n",
    "        'rt_mahalanobis':        mahal,\n",
    "    }, index=df.index)\n",
    "\n",
    "print('Computing train RT geometry...')\n",
    "train_rt = compute_rt_geometry(train_feat, SENSORS_15, fleet_scaler, fleet_pca)\n",
    "print(f'  centroid_dist: mean={train_rt[\"rt_centroid_dist\"].mean():.3f}, '\n",
    "      f'max={train_rt[\"rt_centroid_dist\"].max():.3f}')\n",
    "\n",
    "print('Computing test RT geometry (same fleet baseline)...')\n",
    "test_rt = compute_rt_geometry(test_feat, SENSORS_15, fleet_scaler, fleet_pca)\n",
    "print(f'  centroid_dist: mean={test_rt[\"rt_centroid_dist\"].mean():.3f}, '\n",
    "      f'max={test_rt[\"rt_centroid_dist\"].max():.3f}')"
   ],
   "id": "8e2caf6e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ORTHON Features \u2014 asof Join (window \u2192 cycle)\n",
    "\n",
    "Cohort-level features from `ml/` have one row per (cohort, window).  \n",
    "`signal_0_end` = last cycle in that window.\n",
    "\n",
    "For each (cohort, cycle): take the ORTHON row with the largest `signal_0_end \u2264 cycle`.  \n",
    "This is strictly backward-looking \u2014 we only use context that was available before cycle t."
   ],
   "id": "41139fe5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORTHON files that have signal_0_end for asof join\n",
    "ORTHON_COHORT_FILES = [\n",
    "    ('ml_eigendecomp_derivatives.parquet', 'ed'),  # eigendecomp + d1/d2\n",
    "    ('ml_persistent_homology.parquet',     'ph'),  # topological features\n",
    "]\n",
    "\n",
    "# Select centroid columns: key physical features + d1/d2 (avoid 300-column blast)\n",
    "CENTROID_COLS_SELECT = [\n",
    "    # Trend & regime\n",
    "    'centroid_trend_slope', 'centroid_trend_r2', 'centroid_trend_cusum_range',\n",
    "    'centroid_variance_growth_rate', 'centroid_variance_growth_ratio',\n",
    "    # Complexity / chaos\n",
    "    'centroid_hurst_exponent', 'centroid_lyapunov_exponent',\n",
    "    'centroid_complexity_sample_entropy', 'centroid_complexity_permutation_entropy',\n",
    "    'centroid_correlation_dimension_value',\n",
    "    # Frequency\n",
    "    'centroid_hilbert_freq_drift', 'centroid_spectral_entropy', 'centroid_spectral_slope',\n",
    "    # Spread\n",
    "    'dispersion_mean', 'dispersion_max', 'dispersion_std',\n",
    "    # Attractor\n",
    "    'centroid_rqa_determinism', 'centroid_rqa_recurrence_rate',\n",
    "]\n",
    "\n",
    "def load_orthon_cohort(ml_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and merge cohort-level ORTHON features that have signal_0_end.\"\"\"\n",
    "    frames = []\n",
    "\n",
    "    # Eigendecomp derivatives (eigendecomp + d1/d2)\n",
    "    p = ml_dir / 'ml_eigendecomp_derivatives.parquet'\n",
    "    if p.exists():\n",
    "        df = pl.read_parquet(str(p)).to_pandas()\n",
    "        drop = ['signal_0_start', 'signal_0_center', 'n_signals', 'n_features', 'n_features_valid',\n",
    "                'window_index']\n",
    "        df = df.drop(columns=[c for c in drop if c in df.columns])\n",
    "        df.columns = [f'ed_{c}' if c not in ['cohort', 'signal_0_end'] else c for c in df.columns]\n",
    "        frames.append(df)\n",
    "        print(f'  eigendecomp_derivatives: {len(df)} rows, {df.shape[1]-2} features')\n",
    "\n",
    "    # Persistent homology\n",
    "    p = ml_dir / 'ml_persistent_homology.parquet'\n",
    "    if p.exists():\n",
    "        df = pl.read_parquet(str(p)).to_pandas()\n",
    "        drop = ['window_index', 'n_points']\n",
    "        df = df.drop(columns=[c for c in drop if c in df.columns])\n",
    "        df.columns = [f'ph_{c}' if c not in ['cohort', 'signal_0_end'] else c for c in df.columns]\n",
    "        frames.append(df)\n",
    "        print(f'  persistent_homology: {len(df)} rows, {df.shape[1]-2} features')\n",
    "\n",
    "    # Centroid (curated selection + d1/d2)\n",
    "    p = ml_dir / 'ml_centroid_derivatives.parquet'\n",
    "    if p.exists():\n",
    "        df = pl.read_parquet(str(p)).to_pandas()\n",
    "        # Build list: base + d1 + d2 for selected columns, plus signal_0_end, cohort\n",
    "        keep_base = [c for c in CENTROID_COLS_SELECT if c in df.columns]\n",
    "        keep_d1   = [f'{c}_d1' for c in keep_base if f'{c}_d1' in df.columns]\n",
    "        keep_d2   = [f'{c}_d2' for c in keep_base if f'{c}_d2' in df.columns]\n",
    "        keep_all  = ['cohort', 'signal_0_end'] + keep_base + keep_d1 + keep_d2\n",
    "        df = df[[c for c in keep_all if c in df.columns]]\n",
    "        df.columns = [f'cv_{c}' if c not in ['cohort', 'signal_0_end'] else c for c in df.columns]\n",
    "        frames.append(df)\n",
    "        print(f'  centroid (curated): {len(df)} rows, {df.shape[1]-2} features')\n",
    "\n",
    "    if not frames:\n",
    "        return None\n",
    "\n",
    "    # Merge all on (cohort, signal_0_end)\n",
    "    merged = frames[0]\n",
    "    for f in frames[1:]:\n",
    "        merged = merged.merge(f, on=['cohort', 'signal_0_end'], how='outer')\n",
    "    return merged.sort_values(['cohort', 'signal_0_end'])\n",
    "\n",
    "print('Loading train ORTHON cohort features...')\n",
    "train_orthon = load_orthon_cohort(TRAIN_ML)\n",
    "print(f'  Combined: {train_orthon.shape}')\n",
    "\n",
    "print('Loading test ORTHON cohort features...')\n",
    "test_orthon = load_orthon_cohort(TEST_ML)\n",
    "print(f'  Combined: {test_orthon.shape}')"
   ],
   "id": "c6dda580"
  },
  {
   "cell_type": "code",
   "id": "e20a6a45",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": "## 5b. Fleet Normalization of ORTHON Features\n# Per-cohort delta normalization:\n#   1. For each cohort, record first-window values as that engine's healthy baseline\n#   2. For each window: delta = current - first_window  (drift from own healthy state)\n#   3. Scale by fleet_std (from first windows of all training engines)\n#\n# This removes inter-engine absolute differences while preserving intra-engine dynamics.\n# Same principle as RT geometry: \"how far has this engine drifted from where it started?\"\n#\n# prim_ and traj_ broadcast features are DROPPED \u2014 they are static per-engine fingerprints\n# with no time dimension and cannot be fleet-normalized meaningfully.\n\ndef normalize_orthon_per_cohort(train_df, test_df):\n    \"\"\"\n    Per-cohort delta normalization for ORTHON cohort-level features.\n    Fleet std is computed from first windows of training engines (healthy reference).\n    \"\"\"\n    key_cols = ['cohort', 'signal_0_end']\n    feat_cols = [c for c in train_df.columns if c not in key_cols]\n\n    # Fleet first-window: one row per training engine at its earliest observed window\n    first_idx  = train_df.groupby('cohort')['signal_0_end'].idxmin()\n    fleet_first = train_df.loc[first_idx].set_index('cohort')[feat_cols]\n    fleet_std   = fleet_first.std().clip(lower=1e-8)\n\n    def apply_delta(df, is_test=False):\n        result = df.copy()\n        for cohort_id, grp in df.groupby('cohort', sort=False):\n            if cohort_id in fleet_first.index:\n                baseline = fleet_first.loc[cohort_id, feat_cols].values\n            else:\n                # Test engine not in train: use its own first window as baseline\n                own_first = grp.loc[grp['signal_0_end'].idxmin(), feat_cols].values\n                baseline  = own_first\n            delta = grp[feat_cols].values - baseline\n            result.loc[grp.index, feat_cols] = delta / fleet_std.values\n        return result\n\n    train_norm = apply_delta(train_df)\n    test_norm  = apply_delta(test_df, is_test=True)\n    return train_norm, test_norm, fleet_first, fleet_std\n\nprint('Normalizing train ORTHON features (per-cohort delta from first window)...')\ntrain_orthon_norm, test_orthon_norm, fleet_first_orthon, fleet_std_orthon = \\\n    normalize_orthon_per_cohort(train_orthon, test_orthon)\n\n# Sanity check: first-window deltas should be ~0 for training engines\nfirst_idx  = train_orthon_norm.groupby('cohort')['signal_0_end'].idxmin()\nfirst_vals = train_orthon_norm.loc[first_idx]\nfeat_cols_check = [c for c in train_orthon_norm.columns if c not in ['cohort','signal_0_end']]\nmean_first = first_vals[feat_cols_check].mean().abs().mean()\nprint(f'  Mean |first-window delta| (should be ~0): {mean_first:.6f}')\nprint(f'  ORTHON norm shape: {train_orthon_norm.shape}')\n# Show a few feature ranges after normalization\nfor f in ['ed_effective_dim', 'ed_eigenvalue_0', 'ph_betti_1']:\n    if f in train_orthon_norm.columns:\n        tr_r = train_orthon_norm[f].agg(['min','max','std'])\n        te_r = test_orthon_norm[f].agg(['min','max','std'])\n        print(f'  {f:30s}: train=[{tr_r[\"min\"]:.2f},{tr_r[\"max\"]:.2f}] std={tr_r[\"std\"]:.2f}  ' +\n              f'test=[{te_r[\"min\"]:.2f},{te_r[\"max\"]:.2f}] std={te_r[\"std\"]:.2f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def asof_join_orthon(cycle_df: pd.DataFrame, orthon_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Asof join per cohort: cycles reset (0-361) per engine so merge_asof needs per-group processing.\"\"\"\n    orthon_cols = [c for c in orthon_df.columns if c not in ['cohort', 'signal_0_end']]\n    results = []\n    for cohort_id, grp in cycle_df.groupby('cohort', sort=False):\n        grp_s = grp.sort_values('cycle').reset_index(drop=True)\n        co    = orthon_df[orthon_df['cohort'] == cohort_id].sort_values('signal_0_end')\n        if len(co) == 0:\n            for col in orthon_cols:\n                grp_s[col] = np.nan\n            results.append(grp_s)\n            continue\n        merged = pd.merge_asof(\n            grp_s,\n            co.drop(columns=['cohort']),\n            left_on='cycle',\n            right_on='signal_0_end',\n            direction='backward'\n        ).drop(columns=['signal_0_end'])\n        results.append(merged)\n    result = pd.concat(results, ignore_index=True)\n    n_nan  = result[orthon_cols[0]].isna().sum() if orthon_cols else 0\n    print(f'  asof join: {n_nan:,} rows ({n_nan/len(result):.1%}) with no prior window')\n    return result\n\nprint('Asof joining train ORTHON...')\ntrain_feat_orthon = asof_join_orthon(train_feat, train_orthon_norm)\nprint(f'  Result: {train_feat_orthon.shape}')\n\nprint('Asof joining test ORTHON...')\ntest_feat_orthon = asof_join_orthon(test_feat, test_orthon_norm)\nprint(f'  Result: {test_feat_orthon.shape}')",
   "id": "5bcf6fce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ORTHON Broadcast Features (signal-level, whole-cohort)\n",
    "\n",
    "Signal primitives (Hurst, entropy) have no time dimension \u2014 they are computed over the full series  \n",
    "per signal per cohort. Pivot wide (one column per signal per feature), then broadcast to all cycles."
   ],
   "id": "36d856f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_orthon_signal_broadcast(ml_dir: Path, sensors: list) -> pd.DataFrame:\n",
    "    \"\"\"Load signal-level ORTHON features, aggregate across signals, return one row per cohort.\"\"\"\n",
    "    p = ml_dir / 'ml_signal_primitives.parquet'\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    df = pl.read_parquet(str(p)).to_pandas()\n",
    "    # Filter to varying sensors only\n",
    "    df = df[df['signal_id'].isin(sensors)]\n",
    "    feat_cols = [c for c in df.columns if c not in ['cohort', 'signal_id']]\n",
    "    # Aggregate: mean + std across signals per cohort\n",
    "    agg = df.groupby('cohort')[feat_cols].agg(['mean', 'std']).reset_index()\n",
    "    agg.columns = ['cohort'] + [f'prim_{c[0]}_{c[1]}' for c in agg.columns[1:]]\n",
    "    print(f'  signal_primitives broadcast: {agg.shape[1]-1} features for {len(agg)} cohorts')\n",
    "    return agg\n",
    "\n",
    "def load_orthon_trajectory(ml_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load trajectory match scores \u2014 one row per cohort, broadcast to all cycles.\"\"\"\n",
    "    p = ml_dir / 'ml_trajectory_match.parquet'\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    df = pl.read_parquet(str(p)).to_pandas()\n",
    "    feat_cols = [c for c in df.columns if c not in ['cohort', 'trajectory_id', 'n_windows']]\n",
    "    df = df.groupby('cohort')[feat_cols].mean().reset_index()\n",
    "    df.columns = ['cohort'] + [f'traj_{c}' for c in feat_cols]\n",
    "    print(f'  trajectory_match broadcast: {df.shape[1]-1} features for {len(df)} cohorts')\n",
    "    return df\n",
    "\n",
    "def broadcast_join(cycle_df: pd.DataFrame, broadcast_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Join a per-cohort feature table to the per-cycle DataFrame.\"\"\"\n",
    "    return cycle_df.merge(broadcast_df, on='cohort', how='left')\n",
    "\n",
    "print('Loading broadcast features (train)...')\n",
    "train_prim = load_orthon_signal_broadcast(TRAIN_ML, SENSORS_15)\n",
    "train_traj = load_orthon_trajectory(TRAIN_ML)\n",
    "\n",
    "print('Loading broadcast features (test)...')\n",
    "test_prim  = load_orthon_signal_broadcast(TEST_ML, SENSORS_15)\n",
    "test_traj  = load_orthon_trajectory(TEST_ML)"
   ],
   "id": "0268d09b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Assemble Full Feature Matrix"
   ],
   "id": "eedf87bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def assemble_features(feat_df, rt_df) -> pd.DataFrame:\n    \"\"\"Combine per-cycle features + RT geometry. ORTHON already joined into feat_df.\n    NOTE: prim_ and traj_ broadcast features are excluded \u2014 they are static per-engine\n    fingerprints that encode engine identity, not degradation.\n    \"\"\"\n    return pd.concat([feat_df.reset_index(drop=True), rt_df.reset_index(drop=True)], axis=1)\n\nprint('Assembling train...')\ntrain_full = assemble_features(train_feat_orthon, train_rt)\nprint(f'  Train full: {train_full.shape}')\n\nprint('Assembling test...')\ntest_full = assemble_features(test_feat_orthon, test_rt)\nprint(f'  Test full:  {test_full.shape}')\n\n# Feature groups for ablation\nMETA_COLS  = ['cohort', 'cycle', 'RUL']\nSENSOR_COLS  = SENSORS_15\nROLL_COLS    = [c for c in train_full.columns if c.startswith('roll_')]\nDELTA_COLS   = [c for c in train_full.columns if c.startswith('delta_')]\nRT_COLS      = [c for c in train_full.columns if c.startswith('rt_')]\nORTHON_COLS  = [c for c in train_full.columns if c.startswith(('ed_','ph_','cv_','prim_','traj_'))]\n# Deduplicate while preserving order (prevents LGB feature importance mismatch)\n_seen = set()\nALL_FEAT_COLS = []\nfor _c in ['cycle'] + SENSOR_COLS + ROLL_COLS + DELTA_COLS + RT_COLS + ORTHON_COLS:\n    if _c in train_full.columns and _c not in _seen:\n        ALL_FEAT_COLS.append(_c)\n        _seen.add(_c)\n\nprint(f'\\nFeature groups:')\nprint(f'  cycle:   1')\nprint(f'  sensors: {len(SENSOR_COLS)}')\nprint(f'  rolling: {len(ROLL_COLS)}')\nprint(f'  delta:   {len(DELTA_COLS)}')\nprint(f'  RT geom: {len(RT_COLS)}')\nprint(f'  ORTHON:  {len(ORTHON_COLS)}')\nprint(f'  TOTAL:   {len(ALL_FEAT_COLS)}')",
   "id": "d16bbce7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Matrices"
   ],
   "id": "0dd5b4a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_matrix(df, feat_cols, rul_col='RUL', imputer=None, fit_imputer=True):\n",
    "    \"\"\"Extract X, y from DataFrame. Handles inf/nan. Returns (X, y, groups, imputer).\"\"\"\n",
    "    X = df[feat_cols].values.astype(np.float64)\n",
    "    X = np.where(np.isinf(X), np.nan, X)\n",
    "    if fit_imputer:\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X = imputer.fit_transform(X)\n",
    "    else:\n",
    "        X = imputer.transform(X)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y = df[rul_col].values.astype(np.float64) if rul_col in df.columns else None\n",
    "    groups = df['cohort'].values\n",
    "    return X, y, groups, imputer\n",
    "\n",
    "print('Preparing train matrix...')\n",
    "X_train, y_train, groups_train, imputer = prepare_matrix(train_full, ALL_FEAT_COLS)\n",
    "print(f'  X_train: {X_train.shape}')\n",
    "\n",
    "# Test: get last cycle per engine (prediction point)\n",
    "test_cohorts_sorted = sorted(test_full['cohort'].unique(),\n",
    "                              key=lambda x: int(x.split('_')[-1]))\n",
    "last_idx = [test_full[test_full['cohort'] == c]['cycle'].idxmax() for c in test_cohorts_sorted]\n",
    "test_last = test_full.loc[last_idx].reset_index(drop=True)\n",
    "X_test, _, _, _ = prepare_matrix(test_last, ALL_FEAT_COLS, rul_col='RUL',\n",
    "                                  imputer=imputer, fit_imputer=False)\n",
    "y_test = np.clip(rul_gt, 0, RUL_CAP)\n",
    "print(f'  X_test:  {X_test.shape} ({len(test_cohorts_sorted)} engines)')"
   ],
   "id": "b5ef72a0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run A \u2014 Standard Models (Ridge / RF / GB)\n",
    "\n",
    "Same architecture as v2 for apple-to-apple comparison."
   ],
   "id": "9c18cef1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def phm08(pred, true):\n    d = pred - true\n    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n\ndef evaluate(name, y_true, y_pred, label=''):\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae  = mean_absolute_error(y_true, y_pred)\n    r2   = r2_score(y_true, y_pred)\n    phm  = phm08(y_pred, y_true)\n    bias = np.mean(y_pred - y_true)\n    print(f'  {name:25s} {label}  RMSE={rmse:.2f}  MAE={mae:.2f}  R\u00b2={r2:.4f}  PHM={phm:,.0f}  bias={bias:+.2f}')\n    return {'rmse': rmse, 'mae': mae, 'r2': r2, 'phm08': phm, 'bias': bias}\n\ngkf = GroupKFold(n_splits=N_FOLDS)\nscaler_A = StandardScaler()\nX_train_s = scaler_A.fit_transform(X_train)\nX_test_s  = scaler_A.transform(X_test)\n\n# Run A: Ridge only (fast comparison with v2 Ridge=25.75)\nresultsA_cv   = {}\nresultsA_test = {}\nprint('=== Run A \u2014 Ridge (OOF + Test) ===')\nridge = Ridge(alpha=1.0)\noof_r = np.clip(cross_val_predict(ridge, X_train_s, y_train, groups=groups_train, cv=gkf, n_jobs=-1), 0, RUL_CAP)\nresultsA_cv['ridge'] = evaluate('ridge', y_train, oof_r, '[OOF]')\nridge.fit(X_train_s, y_train)\npred_r = np.clip(ridge.predict(X_test_s), 0, RUL_CAP)\nresultsA_test['ridge'] = evaluate('ridge', y_test, pred_r, '[TEST]')",
   "id": "96697f05"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run B \u2014 Stacking Ensemble (LGB + XGB + Hist \u2192 RidgeCV)\n",
    "\n",
    "Matches the architecture that produced the 11.72 benchmark."
   ],
   "id": "1fd6a2be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Base learners\nlgb_model  = LGBMRegressor(n_estimators=500, max_depth=6, learning_rate=0.05,\n                            num_leaves=63, subsample=0.8, colsample_bytree=0.8,\n                            min_child_samples=20, random_state=SEED, n_jobs=-1,\n                            verbose=-1)\nxgb_model  = XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.05,\n                           subsample=0.8, colsample_bytree=0.8,\n                           min_child_weight=5, random_state=SEED, n_jobs=-1,\n                           verbosity=0)\nhist_model = HistGradientBoostingRegressor(max_iter=500, max_depth=6, learning_rate=0.05,\n                                            min_samples_leaf=20, random_state=SEED)\n\n# Generate OOF predictions \u2014 use n_estimators=300 for speed, 500 for final models\nprint('Generating OOF predictions for stacking meta-features...')\nlgb_oof_m  = LGBMRegressor(n_estimators=300, max_depth=6, learning_rate=0.05, num_leaves=63, subsample=0.8, colsample_bytree=0.8, min_child_samples=20, random_state=SEED, n_jobs=-1, verbose=-1)\nxgb_oof_m  = XGBRegressor(n_estimators=300, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, min_child_weight=5, random_state=SEED, n_jobs=-1, verbosity=0)\nhist_oof_m = HistGradientBoostingRegressor(max_iter=300, max_depth=6, learning_rate=0.05, min_samples_leaf=20, random_state=SEED)\nlgb_oof  = cross_val_predict(lgb_oof_m,  X_train, y_train, groups=groups_train, cv=gkf, n_jobs=1)\nxgb_oof  = cross_val_predict(xgb_oof_m,  X_train, y_train, groups=groups_train, cv=gkf, n_jobs=1)\nhist_oof = cross_val_predict(hist_oof_m, X_train, y_train, groups=groups_train, cv=gkf, n_jobs=1)\n\nlgb_oof  = np.clip(lgb_oof,  0, RUL_CAP)\nxgb_oof  = np.clip(xgb_oof,  0, RUL_CAP)\nhist_oof = np.clip(hist_oof, 0, RUL_CAP)\n\nprint('\\n=== Run B \u2014 OOF (individual base learners) ===')\nevaluate('lgb',  y_train, lgb_oof,  '[OOF]')\nevaluate('xgb',  y_train, xgb_oof,  '[OOF]')\nevaluate('hist', y_train, hist_oof, '[OOF]')\n\n# Meta-learner on OOF\nmeta_X_oof = np.column_stack([lgb_oof, xgb_oof, hist_oof])\nmeta = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0, 100.0])\nmeta.fit(meta_X_oof, y_train)\nstack_oof = np.clip(meta.predict(meta_X_oof), 0, RUL_CAP)\noof_gap = np.sqrt(mean_squared_error(y_train, stack_oof))\n\nprint(f'\\nMeta RidgeCV selected alpha: {meta.alpha_}')\nprint(f'Meta weights: LGB={meta.coef_[0]:.3f}, XGB={meta.coef_[1]:.3f}, Hist={meta.coef_[2]:.3f}')\nevaluate('stacking', y_train, stack_oof, '[OOF]')",
   "id": "ffe96080"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final base learners on full train set\n",
    "print('Training final base learners on full train set...')\n",
    "lgb_final  = lgb_model.__class__(**lgb_model.get_params()).fit(X_train,  y_train)\n",
    "xgb_final  = xgb_model.__class__(**xgb_model.get_params()).fit(X_train,  y_train)\n",
    "hist_final = hist_model.__class__(**hist_model.get_params()).fit(X_train, y_train)\n",
    "\n",
    "# Test predictions\n",
    "lgb_test  = np.clip(lgb_final.predict(X_test),   0, RUL_CAP)\n",
    "xgb_test  = np.clip(xgb_final.predict(X_test),   0, RUL_CAP)\n",
    "hist_test = np.clip(hist_final.predict(X_test),   0, RUL_CAP)\n",
    "meta_test = np.column_stack([lgb_test, xgb_test, hist_test])\n",
    "stack_test = np.clip(meta.predict(meta_test), 0, RUL_CAP)\n",
    "\n",
    "print('\\n=== Run B \u2014 Test (individual base learners) ===')\n",
    "resultsB_test = {}\n",
    "resultsB_test['lgb']  = evaluate('lgb',      y_test, lgb_test,   '[TEST]')\n",
    "resultsB_test['xgb']  = evaluate('xgb',      y_test, xgb_test,   '[TEST]')\n",
    "resultsB_test['hist'] = evaluate('hist',      y_test, hist_test,  '[TEST]')\n",
    "resultsB_test['stacking'] = evaluate('stacking', y_test, stack_test, '[TEST]')\n",
    "\n",
    "test_rmse = resultsB_test['stacking']['rmse']\n",
    "oof_rmse  = oof_gap\n",
    "gap       = test_rmse - oof_rmse\n",
    "print(f'\\nOOF RMSE: {oof_rmse:.2f}  Test RMSE: {test_rmse:.2f}  Gap: {gap:+.2f}')"
   ],
   "id": "b5659697"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance (LGB on full train)"
   ],
   "id": "2f01619f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "importances = lgb_final.feature_importances_\nfeat_names  = np.array(ALL_FEAT_COLS)\norder = np.argsort(importances)[::-1]\n\nprint(f'{\"Rank\":>4s}  {\"Feature\":>55s}  {\"Importance\":>10s}  {\"Layer\":>8s}')\nprint('-' * 82)\nfor rank, i in enumerate(order[:30]):\n    name = feat_names[i]\n    if name == 'cycle':              layer = 'cycle'\n    elif name in SENSORS_15:         layer = 'sensor'\n    elif name.startswith('roll_'):   layer = 'rolling'\n    elif name.startswith('delta_'):  layer = 'delta'\n    elif name.startswith('rt_'):     layer = 'RT_geom'\n    elif name.startswith('ed_'):     layer = 'eigenD'\n    elif name.startswith('ph_'):     layer = 'homology'\n    elif name.startswith('cv_'):     layer = 'centroid'\n    elif name.startswith('prim_'):   layer = 'primitives'\n    elif name.startswith('traj_'):   layer = 'traj'\n    else:                            layer = '?'\n    print(f'{rank+1:>4d}  {name:>55s}  {importances[i]:>10.0f}  {layer:>8s}')\n\nprint(f'feat_names: {len(feat_names)}, importances: {len(importances)}')\n# Sync feat_names to importances length in case of internal LGB deduplication\nfeat_names_trim = feat_names[:len(importances)]\n\n# Layer-level total importance\nprint('\\n=== Layer importance (total) ===')\nlayers = {\n    'cycle':    ['cycle'],\n    'sensors':  SENSORS_15,\n    'rolling':  ROLL_COLS,\n    'delta':    DELTA_COLS,\n    'RT_geom':  RT_COLS,\n    'ORTHON':   ORTHON_COLS,\n}\ntotal_imp = importances.sum()\nfor layer_name, cols in layers.items():\n    cols_set = set(cols)\n    idx = [i for i, c in enumerate(feat_names_trim) if c in cols_set]\n    layer_imp = importances[idx].sum() if idx else 0\n    print(f'  {layer_name:12s}: {layer_imp:8.0f}  ({layer_imp/total_imp:.1%})')",
   "id": "83cc4653"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Ablation \u2014 What Does Each Layer Add?"
   ],
   "id": "a44b15bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_lgb(X_tr, y_tr, X_te, y_te, groups, gkf, rul_cap=125):\n",
    "    \"\"\"Quick LGB OOF + test eval.\"\"\"\n",
    "    model = LGBMRegressor(n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "                          random_state=SEED, n_jobs=-1, verbose=-1)\n",
    "    oof = np.clip(cross_val_predict(model, X_tr, y_tr, groups=groups, cv=gkf, n_jobs=1), 0, rul_cap)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    test_pred = np.clip(model.predict(X_te), 0, rul_cap)\n",
    "    oof_rmse  = np.sqrt(mean_squared_error(y_tr, oof))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_te, test_pred))\n",
    "    return oof_rmse, test_rmse, test_rmse - oof_rmse\n",
    "\n",
    "ablation_sets = [\n",
    "    ('cycle + sensors',                  ['cycle'] + SENSORS_15),\n",
    "    ('+ rolling (CSV baseline)',          ['cycle'] + SENSORS_15 + ROLL_COLS + DELTA_COLS),\n",
    "    ('+ RT geometry',                     ['cycle'] + SENSORS_15 + ROLL_COLS + DELTA_COLS + RT_COLS),\n",
    "    ('+ ORTHON only (no RT)',             ['cycle'] + SENSORS_15 + ROLL_COLS + DELTA_COLS + ORTHON_COLS),\n",
    "    ('Config 4: + RT + ORTHON',           ALL_FEAT_COLS),\n",
    "]\n",
    "\n",
    "print(f'{\"Configuration\":45s}  {\"OOF\":>7s}  {\"Test\":>7s}  {\"Gap\":>7s}')\n",
    "print('-' * 70)\n",
    "ablation_results = {}\n",
    "for label, cols in ablation_sets:\n",
    "    cols_avail = [c for c in cols if c in train_full.columns]\n",
    "    X_tr_abl, y_tr_abl, grps, imp_abl = prepare_matrix(train_full, cols_avail)\n",
    "    X_te_abl, _, _, _                  = prepare_matrix(test_last, cols_avail, imputer=imp_abl, fit_imputer=False)\n",
    "    oof_r, tst_r, gap_r = quick_lgb(X_tr_abl, y_train, X_te_abl, y_test, groups_train, gkf)\n",
    "    print(f'{label:45s}  {oof_r:7.2f}  {tst_r:7.2f}  {gap_r:+7.2f}')\n",
    "    ablation_results[label] = {'oof_rmse': oof_r, 'test_rmse': tst_r, 'gap': gap_r}"
   ],
   "id": "b7eb9675"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary Table"
   ],
   "id": "2db5813d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('RESULTS SUMMARY \u2014 FD001 RUL Prediction')\n",
    "print('=' * 80)\n",
    "print(f'{\"Config\":40s}  {\"RMSE\":>7s}  {\"PHM08\":>9s}  {\"Gap\":>7s}  {\"Features\"}')\n",
    "print('-' * 80)\n",
    "\n",
    "benchmarks = [\n",
    "    ('CSV Standalone (prior)',             12.16, 224,  '+1.05', '~1,044'),\n",
    "    ('ORTHON Alone v2 (prior)',            16.31, 382,  '+1.07', '14'),\n",
    "    ('ORTHON+CSV v3 window-level (prior)', 15.03, 374,  '+0.92', '38'),\n",
    "    ('RT Geometry baseline (prior 11.72)', 11.72, 188,  '-0.70', '413'),\n",
    "]\n",
    "for name, rmse, phm, gap, feats in benchmarks:\n",
    "    print(f'{name:40s}  {rmse:7.2f}  {phm:9,}  {gap:>7s}  {feats}')\n",
    "\n",
    "print('-' * 80)\n",
    "# Run A best\n",
    "best_A = min(resultsA_test, key=lambda k: resultsA_test[k]['rmse'])\n",
    "r = resultsA_test[best_A]\n",
    "gap_A = r['rmse'] - min(resultsA_cv[best_A]['rmse'], r['rmse'])  # approximate\n",
    "print(f'{\"Config 4 Run A (\" + best_A + \")\": <40s}  {r[\"rmse\"]:7.2f}  {r[\"phm08\"]:9,.0f}  {r[\"bias\"]:+7.2f}  {len(ALL_FEAT_COLS)}')\n",
    "\n",
    "# Run B stacking\n",
    "r = resultsB_test['stacking']\n",
    "gap_B = test_rmse - oof_rmse\n",
    "print(f'{\"Config 4 Run B (stacking)\":40s}  {r[\"rmse\"]:7.2f}  {r[\"phm08\"]:9,.0f}  {gap_B:+7.2f}  {len(ALL_FEAT_COLS)}')\n",
    "print('=' * 80)\n",
    "\n",
    "best_rmse = r['rmse']\n",
    "vs_benchmark = best_rmse - 11.72\n",
    "print(f'\\nConfig 4 stacking vs 11.72 benchmark: {vs_benchmark:+.2f} RMSE ({\"BEAT\" if vs_benchmark < 0 else \"missed by\"} {abs(vs_benchmark):.2f})')"
   ],
   "id": "606d032a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Results"
   ],
   "id": "7db19bbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = TRAIN_BASE / 'output_time/ml_results_config4'\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'config4_per_cycle_rt_orthon',\n",
    "    'dataset':    'FD001',\n",
    "    'n_train_rows': int(X_train.shape[0]),\n",
    "    'n_features': int(X_train.shape[1]),\n",
    "    'feature_groups': {\n",
    "        'cycle': 1,\n",
    "        'sensors': len(SENSOR_COLS),\n",
    "        'rolling': len(ROLL_COLS),\n",
    "        'delta': len(DELTA_COLS),\n",
    "        'rt_geometry': len(RT_COLS),\n",
    "        'orthon': len(ORTHON_COLS),\n",
    "    },\n",
    "    'fleet_baseline': {\n",
    "        'early_life_cycles': EARLY_LIFE,\n",
    "        'n_pca_components': N_PCA,\n",
    "        'pc1_explained': float(fleet_pca.explained_variance_ratio_[0]),\n",
    "    },\n",
    "    'run_a_test': resultsA_test,\n",
    "    'run_b_test': resultsB_test,\n",
    "    'run_b_oof_rmse': float(oof_rmse),\n",
    "    'run_b_gap': float(gap_B),\n",
    "    'ablation': ablation_results,\n",
    "    'benchmark_11_72': 11.72,\n",
    "    'delta_vs_benchmark': float(vs_benchmark),\n",
    "    'beat_benchmark': bool(vs_benchmark < 0),\n",
    "}\n",
    "\n",
    "with open(out_dir / 'summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f'Saved to {out_dir}/summary.json')\n",
    "print(json.dumps({k: v for k, v in summary.items() if k not in ['ablation']}, indent=2, default=str))"
   ],
   "id": "874d2e04"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}