{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac527091",
   "metadata": {},
   "source": [
    "# FD001 RUL Prediction — Config 4: Per-Cycle + RT Geometry + ORTHON\n",
    "\n",
    "**Target benchmark:** 11.72 RMSE (the best prior result)\n",
    "\n",
    "**Architecture:**\n",
    "- Layer 1: Cycle number (1)\n",
    "- Layer 2: Raw sensors — 15 varying (15)\n",
    "- Layer 3: Rolling stats — 15 sensors × 5 windows × 5 stats (375)\n",
    "- Layer 4: Delta features — sensor[t] - sensor[t-1] (15)\n",
    "- Layer 5: RT geometry — fleet healthy baseline, per cycle (5)\n",
    "- Layer 6: ORTHON features — eigendecomp derivatives + homology + trajectory, asof-joined from ml/ (window → cycle)\n",
    "\n",
    "**Models:**\n",
    "- Run A: Ridge / RandomForest / GradientBoosting (comparable to v2)\n",
    "- Run B: LGB + XGB + HistGBM → RidgeCV stacking (matches 11.72 architecture)\n",
    "\n",
    "**Key constraint:** Fleet baseline fitted on TRAIN only, applied identically to TRAIN and TEST.\n",
    "All features are strictly backward-looking (no future lookahead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31072682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:32:58.116581Z",
     "iopub.status.busy": "2026-02-25T23:32:58.116421Z",
     "iopub.status.idle": "2026-02-25T23:32:59.160223Z",
     "shell.execute_reply": "2026-02-25T23:32:59.159745Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b92f1d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:32:59.161515Z",
     "iopub.status.busy": "2026-02-25T23:32:59.161399Z",
     "iopub.status.idle": "2026-02-25T23:32:59.163869Z",
     "shell.execute_reply": "2026-02-25T23:32:59.163441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready.\n",
      "  15 sensors, 5 roll windows\n",
      "  Rolling features: 375 = 15 × 5 × 5 stats\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "TRAIN_BASE = Path('/Users/jasonrudder/domains/cmapss/FD_001/Train')\n",
    "TEST_BASE  = Path('/Users/jasonrudder/domains/cmapss/FD_001/Test')\n",
    "TRAIN_ML   = TRAIN_BASE / 'output_time/ml'\n",
    "TEST_ML    = TEST_BASE  / 'output_time/ml'\n",
    "RUL_PATH   = Path('/Users/jasonrudder/domains/cmapss/FD_001/RUL_FD001.txt')\n",
    "\n",
    "# The 15 sensors used in the 11.72 benchmark (excluding 7 constants + op1/op2 operational settings)\n",
    "SENSORS_15 = ['BPR', 'NRc', 'NRf', 'Nc', 'Nf', 'P15', 'P30', 'Ps30',\n",
    "               'T24', 'T30', 'T50', 'W31', 'W32', 'htBleed', 'phi']\n",
    "\n",
    "ROLL_WINDOWS  = [5, 10, 15, 20, 30]\n",
    "EARLY_LIFE    = 30     # first N cycles define fleet healthy baseline\n",
    "N_PCA         = 10     # PCA components for fleet baseline\n",
    "RUL_CAP       = 125\n",
    "N_FOLDS       = 5\n",
    "SEED          = 42\n",
    "\n",
    "print('Config ready.')\n",
    "print(f'  {len(SENSORS_15)} sensors, {len(ROLL_WINDOWS)} roll windows')\n",
    "print(f'  Rolling features: {len(SENSORS_15) * len(ROLL_WINDOWS) * 5} = {len(SENSORS_15)} × {len(ROLL_WINDOWS)} × 5 stats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562385b9",
   "metadata": {},
   "source": [
    "## 1. Load Observations → Wide Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb9e474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:32:59.164882Z",
     "iopub.status.busy": "2026-02-25T23:32:59.164816Z",
     "iopub.status.idle": "2026-02-25T23:32:59.192082Z",
     "shell.execute_reply": "2026-02-25T23:32:59.191761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train...\n",
      "  Train: (20631, 17) — 100 engines\n",
      "Loading test...\n",
      "  Test:  (13096, 17) — 100 engines\n"
     ]
    }
   ],
   "source": [
    "def load_wide(obs_path: Path, sensors: list) -> pd.DataFrame:\n",
    "    \"\"\"Load observations.parquet, filter to sensors, pivot to one row per (cohort, cycle).\"\"\"\n",
    "    obs = pl.read_parquet(str(obs_path))\n",
    "    obs_s = obs.filter(pl.col('signal_id').is_in(sensors))\n",
    "    wide = (\n",
    "        obs_s\n",
    "        .pivot(index=['cohort', 'signal_0'], on='signal_id', values='value',\n",
    "               aggregate_function='first')\n",
    "        .sort(['cohort', 'signal_0'])\n",
    "    )\n",
    "    df = wide.to_pandas().rename(columns={'signal_0': 'cycle'})\n",
    "    return df\n",
    "\n",
    "print('Loading train...')\n",
    "train_wide = load_wide(TRAIN_BASE / 'observations.parquet', SENSORS_15)\n",
    "print(f'  Train: {train_wide.shape} — {train_wide[\"cohort\"].nunique()} engines')\n",
    "\n",
    "print('Loading test...')\n",
    "test_wide = load_wide(TEST_BASE / 'observations.parquet', SENSORS_15)\n",
    "print(f'  Test:  {test_wide.shape} — {test_wide[\"cohort\"].nunique()} engines')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c8a97",
   "metadata": {},
   "source": [
    "## 2. RUL Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f979685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:32:59.193089Z",
     "iopub.status.busy": "2026-02-25T23:32:59.193024Z",
     "iopub.status.idle": "2026-02-25T23:32:59.196975Z",
     "shell.execute_reply": "2026-02-25T23:32:59.196665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RUL: 0 – 125 (20631 rows)\n",
      "Test GT RUL: 7 – 145 (100 engines)\n"
     ]
    }
   ],
   "source": [
    "def add_train_rul(df: pd.DataFrame, cap: int = 125) -> pd.DataFrame:\n",
    "    \"\"\"RUL = max_cycle_per_engine - current_cycle, capped.\"\"\"\n",
    "    df = df.copy()\n",
    "    max_cycle = df.groupby('cohort')['cycle'].transform('max')\n",
    "    df['RUL'] = np.clip((max_cycle - df['cycle']).values, 0, cap)\n",
    "    return df\n",
    "\n",
    "train_wide = add_train_rul(train_wide, RUL_CAP)\n",
    "\n",
    "# Load test ground truth (ordered by engine number, 1-indexed)\n",
    "rul_gt = np.loadtxt(str(RUL_PATH))\n",
    "print(f'Train RUL: {train_wide[\"RUL\"].min():.0f} – {train_wide[\"RUL\"].max():.0f} ({len(train_wide)} rows)')\n",
    "print(f'Test GT RUL: {rul_gt.min():.0f} – {rul_gt.max():.0f} ({len(rul_gt)} engines)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75b706",
   "metadata": {},
   "source": [
    "## 3. Per-Cycle Features (Rolling + Delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26834046",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:32:59.198035Z",
     "iopub.status.busy": "2026-02-25T23:32:59.197975Z",
     "iopub.status.idle": "2026-02-25T23:33:01.180003Z",
     "shell.execute_reply": "2026-02-25T23:33:01.179599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train per-cycle features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: (20631, 408) — expect ~406 feature columns\n",
      "Building test per-cycle features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test:  (13096, 407)\n"
     ]
    }
   ],
   "source": [
    "def add_per_cycle_features(df: pd.DataFrame, sensors: list, windows: list) -> pd.DataFrame:\n",
    "    \"\"\"Add rolling stats and delta features. Processes per engine to prevent bleed.\"\"\"\n",
    "    df = df.sort_values(['cohort', 'cycle']).copy()\n",
    "    results = []\n",
    "    for cohort_id, grp in df.groupby('cohort', sort=False):\n",
    "        grp = grp.reset_index(drop=True)\n",
    "        # Rolling stats (5 stats × 5 windows × 15 sensors = 375)\n",
    "        new_cols = {}\n",
    "        for win in windows:\n",
    "            r = grp[sensors].rolling(win, min_periods=1)\n",
    "            means = r.mean()\n",
    "            stds  = r.std(ddof=0).fillna(0.0)\n",
    "            mins  = r.min()\n",
    "            maxs  = r.max()\n",
    "            for s in sensors:\n",
    "                new_cols[f'roll_{s}_mean_{win}']  = means[s].values\n",
    "                new_cols[f'roll_{s}_std_{win}']   = stds[s].values\n",
    "                new_cols[f'roll_{s}_min_{win}']   = mins[s].values\n",
    "                new_cols[f'roll_{s}_max_{win}']   = maxs[s].values\n",
    "                new_cols[f'roll_{s}_range_{win}'] = (maxs[s] - mins[s]).values\n",
    "        for s in sensors:\n",
    "            new_cols[f'delta_{s}'] = grp[s].diff(1).fillna(0.0).values\n",
    "        grp = pd.concat([grp, pd.DataFrame(new_cols, index=grp.index)], axis=1)\n",
    "        results.append(grp)\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "print('Building train per-cycle features...')\n",
    "train_feat = add_per_cycle_features(train_wide, SENSORS_15, ROLL_WINDOWS)\n",
    "n_per_cycle = 1 + len(SENSORS_15) + len(SENSORS_15)*len(ROLL_WINDOWS)*5 + len(SENSORS_15)  # cycle+sensors+rolling+delta\n",
    "print(f'  Train: {train_feat.shape} — expect ~{n_per_cycle} feature columns')\n",
    "\n",
    "print('Building test per-cycle features...')\n",
    "test_feat = add_per_cycle_features(test_wide, SENSORS_15, ROLL_WINDOWS)\n",
    "print(f'  Test:  {test_feat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4240df8",
   "metadata": {},
   "source": [
    "## 4. Fleet Baseline + RT Geometry\n",
    "\n",
    "Fleet centroid = mean of the first `EARLY_LIFE` cycles pooled across ALL training engines.  \n",
    "Fitted on TRAIN only. Applied identically to train and test (no leakage).\n",
    "\n",
    "5 RT geometry features per cycle:\n",
    "- `rt_centroid_dist` — L2 distance from fleet mean (in standardized space)\n",
    "- `rt_centroid_dist_norm` — distance normalized by √n_sensors\n",
    "- `rt_pc1_proj` — projection onto primary degradation axis\n",
    "- `rt_pc2_proj` — projection onto secondary axis\n",
    "- `rt_mahalanobis` — eigenvalue-weighted distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e32b8ff0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:33:01.181200Z",
     "iopub.status.busy": "2026-02-25T23:33:01.181115Z",
     "iopub.status.idle": "2026-02-25T23:33:01.185471Z",
     "shell.execute_reply": "2026-02-25T23:33:01.185097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleet baseline: 3,100 early-life cycles from 100 engines\n",
      "PC1 explains 48.2%, PC2 7.8%, top-5 cumulative 71.7%\n"
     ]
    }
   ],
   "source": [
    "# Build fleet baseline from early-life train cycles\n",
    "early_data = train_wide[train_wide['cycle'] <= EARLY_LIFE][SENSORS_15].dropna()\n",
    "fleet_scaler = StandardScaler()\n",
    "early_scaled = fleet_scaler.fit_transform(early_data.values)\n",
    "fleet_pca = PCA(n_components=N_PCA, random_state=SEED)\n",
    "fleet_pca.fit(early_scaled)\n",
    "\n",
    "n_early_engines = train_wide[train_wide['cycle'] <= EARLY_LIFE]['cohort'].nunique()\n",
    "print(f'Fleet baseline: {len(early_data):,} early-life cycles from {n_early_engines} engines')\n",
    "print(f'PC1 explains {fleet_pca.explained_variance_ratio_[0]:.1%}, '\n",
    "      f'PC2 {fleet_pca.explained_variance_ratio_[1]:.1%}, '\n",
    "      f'top-5 cumulative {fleet_pca.explained_variance_ratio_[:5].sum():.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e2caf6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:33:01.186429Z",
     "iopub.status.busy": "2026-02-25T23:33:01.186374Z",
     "iopub.status.idle": "2026-02-25T23:33:01.192594Z",
     "shell.execute_reply": "2026-02-25T23:33:01.192209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing train RT geometry...\n",
      "  centroid_dist: mean=5.783, max=34.758\n",
      "Computing test RT geometry (same fleet baseline)...\n",
      "  centroid_dist: mean=4.043, max=17.341\n"
     ]
    }
   ],
   "source": [
    "def compute_rt_geometry(df: pd.DataFrame, sensors: list,\n",
    "                        scaler: StandardScaler, pca: PCA) -> pd.DataFrame:\n",
    "    \"\"\"Compute 5 RT geometry features per row using the pre-fitted fleet baseline.\"\"\"\n",
    "    X = df[sensors].values.astype(np.float64)\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    # Centroid distance (fleet centroid = origin in scaled space)\n",
    "    centroid_dist      = np.linalg.norm(X_scaled, axis=1)\n",
    "    centroid_dist_norm = centroid_dist / np.sqrt(X_scaled.shape[1])\n",
    "\n",
    "    # PC projections\n",
    "    X_proj    = pca.transform(X_scaled)        # (n, n_pcs)\n",
    "    pc1_proj  = X_proj[:, 0]\n",
    "    pc2_proj  = X_proj[:, 1] if pca.n_components_ > 1 else np.zeros(len(X_scaled))\n",
    "\n",
    "    # Mahalanobis approximation\n",
    "    lambdas   = np.maximum(pca.explained_variance_, 1e-10)\n",
    "    mahal     = np.sqrt(np.sum(X_proj**2 / lambdas, axis=1))\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'rt_centroid_dist':      centroid_dist,\n",
    "        'rt_centroid_dist_norm': centroid_dist_norm,\n",
    "        'rt_pc1_proj':           pc1_proj,\n",
    "        'rt_pc2_proj':           pc2_proj,\n",
    "        'rt_mahalanobis':        mahal,\n",
    "    }, index=df.index)\n",
    "\n",
    "print('Computing train RT geometry...')\n",
    "train_rt = compute_rt_geometry(train_feat, SENSORS_15, fleet_scaler, fleet_pca)\n",
    "print(f'  centroid_dist: mean={train_rt[\"rt_centroid_dist\"].mean():.3f}, '\n",
    "      f'max={train_rt[\"rt_centroid_dist\"].max():.3f}')\n",
    "\n",
    "print('Computing test RT geometry (same fleet baseline)...')\n",
    "test_rt = compute_rt_geometry(test_feat, SENSORS_15, fleet_scaler, fleet_pca)\n",
    "print(f'  centroid_dist: mean={test_rt[\"rt_centroid_dist\"].mean():.3f}, '\n",
    "      f'max={test_rt[\"rt_centroid_dist\"].max():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41139fe5",
   "metadata": {},
   "source": [
    "## 5. ORTHON Features — asof Join (window → cycle)\n",
    "\n",
    "Cohort-level features from `ml/` have one row per (cohort, window).  \n",
    "`signal_0_end` = last cycle in that window.\n",
    "\n",
    "For each (cohort, cycle): take the ORTHON row with the largest `signal_0_end ≤ cycle`.  \n",
    "This is strictly backward-looking — we only use context that was available before cycle t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6dda580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:33:01.193523Z",
     "iopub.status.busy": "2026-02-25T23:33:01.193460Z",
     "iopub.status.idle": "2026-02-25T23:33:01.227421Z",
     "shell.execute_reply": "2026-02-25T23:33:01.226936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train ORTHON cohort features...\n",
      "  eigendecomp_derivatives: 789 rows, 131 features\n",
      "  persistent_homology: 789 rows, 8 features\n",
      "  centroid (curated): 789 rows, 54 features\n",
      "  Combined: (789, 195)\n",
      "Loading test ORTHON cohort features...\n",
      "  eigendecomp_derivatives: 675 rows, 131 features\n",
      "  persistent_homology: 675 rows, 8 features\n",
      "  centroid (curated): 675 rows, 54 features\n",
      "  Combined: (675, 195)\n"
     ]
    }
   ],
   "source": [
    "# ORTHON files that have signal_0_end for asof join\n",
    "ORTHON_COHORT_FILES = [\n",
    "    ('ml_eigendecomp_derivatives.parquet', 'ed'),  # eigendecomp + d1/d2\n",
    "    ('ml_persistent_homology.parquet',     'ph'),  # topological features\n",
    "]\n",
    "\n",
    "# Select centroid columns: key physical features + d1/d2 (avoid 300-column blast)\n",
    "CENTROID_COLS_SELECT = [\n",
    "    # Trend & regime\n",
    "    'centroid_trend_slope', 'centroid_trend_r2', 'centroid_trend_cusum_range',\n",
    "    'centroid_variance_growth_rate', 'centroid_variance_growth_ratio',\n",
    "    # Complexity / chaos\n",
    "    'centroid_hurst_exponent', 'centroid_lyapunov_exponent',\n",
    "    'centroid_complexity_sample_entropy', 'centroid_complexity_permutation_entropy',\n",
    "    'centroid_correlation_dimension_value',\n",
    "    # Frequency\n",
    "    'centroid_hilbert_freq_drift', 'centroid_spectral_entropy', 'centroid_spectral_slope',\n",
    "    # Spread\n",
    "    'dispersion_mean', 'dispersion_max', 'dispersion_std',\n",
    "    # Attractor\n",
    "    'centroid_rqa_determinism', 'centroid_rqa_recurrence_rate',\n",
    "]\n",
    "\n",
    "def load_orthon_cohort(ml_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and merge cohort-level ORTHON features that have signal_0_end.\"\"\"\n",
    "    frames = []\n",
    "\n",
    "    # Eigendecomp derivatives (eigendecomp + d1/d2)\n",
    "    p = ml_dir / 'ml_eigendecomp_derivatives.parquet'\n",
    "    if p.exists():\n",
    "        df = pl.read_parquet(str(p)).to_pandas()\n",
    "        drop = ['signal_0_start', 'signal_0_center', 'n_signals', 'n_features', 'n_features_valid',\n",
    "                'window_index']\n",
    "        df = df.drop(columns=[c for c in drop if c in df.columns])\n",
    "        df.columns = [f'ed_{c}' if c not in ['cohort', 'signal_0_end'] else c for c in df.columns]\n",
    "        frames.append(df)\n",
    "        print(f'  eigendecomp_derivatives: {len(df)} rows, {df.shape[1]-2} features')\n",
    "\n",
    "    # Persistent homology\n",
    "    p = ml_dir / 'ml_persistent_homology.parquet'\n",
    "    if p.exists():\n",
    "        df = pl.read_parquet(str(p)).to_pandas()\n",
    "        drop = ['window_index', 'n_points']\n",
    "        df = df.drop(columns=[c for c in drop if c in df.columns])\n",
    "        df.columns = [f'ph_{c}' if c not in ['cohort', 'signal_0_end'] else c for c in df.columns]\n",
    "        frames.append(df)\n",
    "        print(f'  persistent_homology: {len(df)} rows, {df.shape[1]-2} features')\n",
    "\n",
    "    # Centroid (curated selection + d1/d2)\n",
    "    p = ml_dir / 'ml_centroid_derivatives.parquet'\n",
    "    if p.exists():\n",
    "        df = pl.read_parquet(str(p)).to_pandas()\n",
    "        # Build list: base + d1 + d2 for selected columns, plus signal_0_end, cohort\n",
    "        keep_base = [c for c in CENTROID_COLS_SELECT if c in df.columns]\n",
    "        keep_d1   = [f'{c}_d1' for c in keep_base if f'{c}_d1' in df.columns]\n",
    "        keep_d2   = [f'{c}_d2' for c in keep_base if f'{c}_d2' in df.columns]\n",
    "        keep_all  = ['cohort', 'signal_0_end'] + keep_base + keep_d1 + keep_d2\n",
    "        df = df[[c for c in keep_all if c in df.columns]]\n",
    "        df.columns = [f'cv_{c}' if c not in ['cohort', 'signal_0_end'] else c for c in df.columns]\n",
    "        frames.append(df)\n",
    "        print(f'  centroid (curated): {len(df)} rows, {df.shape[1]-2} features')\n",
    "\n",
    "    if not frames:\n",
    "        return None\n",
    "\n",
    "    # Merge all on (cohort, signal_0_end)\n",
    "    merged = frames[0]\n",
    "    for f in frames[1:]:\n",
    "        merged = merged.merge(f, on=['cohort', 'signal_0_end'], how='outer')\n",
    "    return merged.sort_values(['cohort', 'signal_0_end'])\n",
    "\n",
    "print('Loading train ORTHON cohort features...')\n",
    "train_orthon = load_orthon_cohort(TRAIN_ML)\n",
    "print(f'  Combined: {train_orthon.shape}')\n",
    "\n",
    "print('Loading test ORTHON cohort features...')\n",
    "test_orthon = load_orthon_cohort(TEST_ML)\n",
    "print(f'  Combined: {test_orthon.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e20a6a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:33:01.228398Z",
     "iopub.status.busy": "2026-02-25T23:33:01.228338Z",
     "iopub.status.idle": "2026-02-25T23:33:02.456862Z",
     "shell.execute_reply": "2026-02-25T23:33:02.456372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing train ORTHON features (per-cohort delta from first window)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean |first-window delta| (should be ~0): 0.000000\n",
      "  ORTHON norm shape: (789, 195)\n",
      "  ed_effective_dim              : train=[-4.48,2.49] std=1.12  test=[-4.26,3.31] std=1.53\n",
      "  ed_eigenvalue_0               : train=[0.00,6.56] std=1.52  test=[-3.13,6.74] std=1.95\n",
      "  ph_betti_1                    : train=[0.00,0.00] std=0.00  test=[0.00,0.00] std=0.00\n"
     ]
    }
   ],
   "source": [
    "## 5b. Fleet Normalization of ORTHON Features\n",
    "# Per-cohort delta normalization:\n",
    "#   1. For each cohort, record first-window values as that engine's healthy baseline\n",
    "#   2. For each window: delta = current - first_window  (drift from own healthy state)\n",
    "#   3. Scale by fleet_std (from first windows of all training engines)\n",
    "#\n",
    "# This removes inter-engine absolute differences while preserving intra-engine dynamics.\n",
    "# Same principle as RT geometry: \"how far has this engine drifted from where it started?\"\n",
    "#\n",
    "# prim_ and traj_ broadcast features are DROPPED — they are static per-engine fingerprints\n",
    "# with no time dimension and cannot be fleet-normalized meaningfully.\n",
    "\n",
    "def normalize_orthon_per_cohort(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Per-cohort delta normalization for ORTHON cohort-level features.\n",
    "    Fleet std is computed from first windows of training engines (healthy reference).\n",
    "    \"\"\"\n",
    "    key_cols = ['cohort', 'signal_0_end']\n",
    "    feat_cols = [c for c in train_df.columns if c not in key_cols]\n",
    "\n",
    "    # Fleet first-window: one row per training engine at its earliest observed window\n",
    "    first_idx  = train_df.groupby('cohort')['signal_0_end'].idxmin()\n",
    "    fleet_first = train_df.loc[first_idx].set_index('cohort')[feat_cols]\n",
    "    fleet_std   = fleet_first.std().clip(lower=1e-8)\n",
    "\n",
    "    def apply_delta(df, is_test=False):\n",
    "        result = df.copy()\n",
    "        for cohort_id, grp in df.groupby('cohort', sort=False):\n",
    "            if cohort_id in fleet_first.index:\n",
    "                baseline = fleet_first.loc[cohort_id, feat_cols].values\n",
    "            else:\n",
    "                # Test engine not in train: use its own first window as baseline\n",
    "                own_first = grp.loc[grp['signal_0_end'].idxmin(), feat_cols].values\n",
    "                baseline  = own_first\n",
    "            delta = grp[feat_cols].values - baseline\n",
    "            result.loc[grp.index, feat_cols] = delta / fleet_std.values\n",
    "        return result\n",
    "\n",
    "    train_norm = apply_delta(train_df)\n",
    "    test_norm  = apply_delta(test_df, is_test=True)\n",
    "    return train_norm, test_norm, fleet_first, fleet_std\n",
    "\n",
    "print('Normalizing train ORTHON features (per-cohort delta from first window)...')\n",
    "train_orthon_norm, test_orthon_norm, fleet_first_orthon, fleet_std_orthon = \\\n",
    "    normalize_orthon_per_cohort(train_orthon, test_orthon)\n",
    "\n",
    "# Sanity check: first-window deltas should be ~0 for training engines\n",
    "first_idx  = train_orthon_norm.groupby('cohort')['signal_0_end'].idxmin()\n",
    "first_vals = train_orthon_norm.loc[first_idx]\n",
    "feat_cols_check = [c for c in train_orthon_norm.columns if c not in ['cohort','signal_0_end']]\n",
    "mean_first = first_vals[feat_cols_check].mean().abs().mean()\n",
    "print(f'  Mean |first-window delta| (should be ~0): {mean_first:.6f}')\n",
    "print(f'  ORTHON norm shape: {train_orthon_norm.shape}')\n",
    "# Show a few feature ranges after normalization\n",
    "for f in ['ed_effective_dim', 'ed_eigenvalue_0', 'ph_betti_1']:\n",
    "    if f in train_orthon_norm.columns:\n",
    "        tr_r = train_orthon_norm[f].agg(['min','max','std'])\n",
    "        te_r = test_orthon_norm[f].agg(['min','max','std'])\n",
    "        print(f'  {f:30s}: train=[{tr_r[\"min\"]:.2f},{tr_r[\"max\"]:.2f}] std={tr_r[\"std\"]:.2f}  ' +\n",
    "              f'test=[{te_r[\"min\"]:.2f},{te_r[\"max\"]:.2f}] std={te_r[\"std\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bcf6fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:33:02.458038Z",
     "iopub.status.busy": "2026-02-25T23:33:02.457961Z",
     "iopub.status.idle": "2026-02-25T23:33:02.713439Z",
     "shell.execute_reply": "2026-02-25T23:33:02.713112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asof joining train ORTHON...\n",
      "  asof join: 4,300 rows (20.8%) with no prior window\n",
      "  Result: (20631, 601)\n",
      "Asof joining test ORTHON...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  asof join: 3,100 rows (23.7%) with no prior window\n",
      "  Result: (13096, 600)\n"
     ]
    }
   ],
   "source": [
    "def asof_join_orthon(cycle_df: pd.DataFrame, orthon_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Asof join per cohort: cycles reset (0-361) per engine so merge_asof needs per-group processing.\"\"\"\n",
    "    orthon_cols = [c for c in orthon_df.columns if c not in ['cohort', 'signal_0_end']]\n",
    "    results = []\n",
    "    for cohort_id, grp in cycle_df.groupby('cohort', sort=False):\n",
    "        grp_s = grp.sort_values('cycle').reset_index(drop=True)\n",
    "        co    = orthon_df[orthon_df['cohort'] == cohort_id].sort_values('signal_0_end')\n",
    "        if len(co) == 0:\n",
    "            for col in orthon_cols:\n",
    "                grp_s[col] = np.nan\n",
    "            results.append(grp_s)\n",
    "            continue\n",
    "        merged = pd.merge_asof(\n",
    "            grp_s,\n",
    "            co.drop(columns=['cohort']),\n",
    "            left_on='cycle',\n",
    "            right_on='signal_0_end',\n",
    "            direction='backward'\n",
    "        ).drop(columns=['signal_0_end'])\n",
    "        results.append(merged)\n",
    "    result = pd.concat(results, ignore_index=True)\n",
    "    n_nan  = result[orthon_cols[0]].isna().sum() if orthon_cols else 0\n",
    "    print(f'  asof join: {n_nan:,} rows ({n_nan/len(result):.1%}) with no prior window')\n",
    "    return result\n",
    "\n",
    "print('Asof joining train ORTHON...')\n",
    "train_feat_orthon = asof_join_orthon(train_feat, train_orthon_norm)\n",
    "print(f'  Result: {train_feat_orthon.shape}')\n",
    "\n",
    "print('Asof joining test ORTHON...')\n",
    "test_feat_orthon = asof_join_orthon(test_feat, test_orthon_norm)\n",
    "print(f'  Result: {test_feat_orthon.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d856f9",
   "metadata": {},
   "source": [
    "## 6. ORTHON Broadcast Features (signal-level, whole-cohort)\n",
    "\n",
    "Signal primitives (Hurst, entropy) have no time dimension — they are computed over the full series  \n",
    "per signal per cohort. Pivot wide (one column per signal per feature), then broadcast to all cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0268d09b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:33:02.714706Z",
     "iopub.status.busy": "2026-02-25T23:33:02.714611Z",
     "iopub.status.idle": "2026-02-25T23:33:02.734499Z",
     "shell.execute_reply": "2026-02-25T23:33:02.734103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading broadcast features (train)...\n",
      "  signal_primitives broadcast: 16 features for 100 cohorts\n",
      "  trajectory_match broadcast: 4 features for 100 cohorts\n",
      "Loading broadcast features (test)...\n",
      "  signal_primitives broadcast: 16 features for 100 cohorts\n",
      "  trajectory_match broadcast: 4 features for 95 cohorts\n"
     ]
    }
   ],
   "source": [
    "def load_orthon_signal_broadcast(ml_dir: Path, sensors: list) -> pd.DataFrame:\n",
    "    \"\"\"Load signal-level ORTHON features, aggregate across signals, return one row per cohort.\"\"\"\n",
    "    p = ml_dir / 'ml_signal_primitives.parquet'\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    df = pl.read_parquet(str(p)).to_pandas()\n",
    "    # Filter to varying sensors only\n",
    "    df = df[df['signal_id'].isin(sensors)]\n",
    "    feat_cols = [c for c in df.columns if c not in ['cohort', 'signal_id']]\n",
    "    # Aggregate: mean + std across signals per cohort\n",
    "    agg = df.groupby('cohort')[feat_cols].agg(['mean', 'std']).reset_index()\n",
    "    agg.columns = ['cohort'] + [f'prim_{c[0]}_{c[1]}' for c in agg.columns[1:]]\n",
    "    print(f'  signal_primitives broadcast: {agg.shape[1]-1} features for {len(agg)} cohorts')\n",
    "    return agg\n",
    "\n",
    "def load_orthon_trajectory(ml_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load trajectory match scores — one row per cohort, broadcast to all cycles.\"\"\"\n",
    "    p = ml_dir / 'ml_trajectory_match.parquet'\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    df = pl.read_parquet(str(p)).to_pandas()\n",
    "    feat_cols = [c for c in df.columns if c not in ['cohort', 'trajectory_id', 'n_windows']]\n",
    "    df = df.groupby('cohort')[feat_cols].mean().reset_index()\n",
    "    df.columns = ['cohort'] + [f'traj_{c}' for c in feat_cols]\n",
    "    print(f'  trajectory_match broadcast: {df.shape[1]-1} features for {len(df)} cohorts')\n",
    "    return df\n",
    "\n",
    "def broadcast_join(cycle_df: pd.DataFrame, broadcast_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Join a per-cohort feature table to the per-cycle DataFrame.\"\"\"\n",
    "    return cycle_df.merge(broadcast_df, on='cohort', how='left')\n",
    "\n",
    "print('Loading broadcast features (train)...')\n",
    "train_prim = load_orthon_signal_broadcast(TRAIN_ML, SENSORS_15)\n",
    "train_traj = load_orthon_trajectory(TRAIN_ML)\n",
    "\n",
    "print('Loading broadcast features (test)...')\n",
    "test_prim  = load_orthon_signal_broadcast(TEST_ML, SENSORS_15)\n",
    "test_traj  = load_orthon_trajectory(TEST_ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf87bf",
   "metadata": {},
   "source": [
    "## 7. Assemble Full Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d16bbce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:33:02.735608Z",
     "iopub.status.busy": "2026-02-25T23:33:02.735544Z",
     "iopub.status.idle": "2026-02-25T23:33:02.740345Z",
     "shell.execute_reply": "2026-02-25T23:33:02.739904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling train...\n",
      "  Train full: (20631, 606)\n",
      "Assembling test...\n",
      "  Test full:  (13096, 605)\n",
      "\n",
      "Feature groups:\n",
      "  cycle:   1\n",
      "  sensors: 15\n",
      "  rolling: 375\n",
      "  delta:   15\n",
      "  RT geom: 5\n",
      "  ORTHON:  193\n",
      "  TOTAL:   604\n"
     ]
    }
   ],
   "source": [
    "def assemble_features(feat_df, rt_df) -> pd.DataFrame:\n",
    "    \"\"\"Combine per-cycle features + RT geometry. ORTHON already joined into feat_df.\n",
    "    NOTE: prim_ and traj_ broadcast features are excluded — they are static per-engine\n",
    "    fingerprints that encode engine identity, not degradation.\n",
    "    \"\"\"\n",
    "    return pd.concat([feat_df.reset_index(drop=True), rt_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print('Assembling train...')\n",
    "train_full = assemble_features(train_feat_orthon, train_rt)\n",
    "print(f'  Train full: {train_full.shape}')\n",
    "\n",
    "print('Assembling test...')\n",
    "test_full = assemble_features(test_feat_orthon, test_rt)\n",
    "print(f'  Test full:  {test_full.shape}')\n",
    "\n",
    "# Feature groups for ablation\n",
    "META_COLS  = ['cohort', 'cycle', 'RUL']\n",
    "SENSOR_COLS  = SENSORS_15\n",
    "ROLL_COLS    = [c for c in train_full.columns if c.startswith('roll_')]\n",
    "DELTA_COLS   = [c for c in train_full.columns if c.startswith('delta_')]\n",
    "RT_COLS      = [c for c in train_full.columns if c.startswith('rt_')]\n",
    "ORTHON_COLS  = [c for c in train_full.columns if c.startswith(('ed_','ph_','cv_','prim_','traj_'))]\n",
    "# Deduplicate while preserving order (prevents LGB feature importance mismatch)\n",
    "_seen = set()\n",
    "ALL_FEAT_COLS = []\n",
    "for _c in ['cycle'] + SENSOR_COLS + ROLL_COLS + DELTA_COLS + RT_COLS + ORTHON_COLS:\n",
    "    if _c in train_full.columns and _c not in _seen:\n",
    "        ALL_FEAT_COLS.append(_c)\n",
    "        _seen.add(_c)\n",
    "\n",
    "print(f'\\nFeature groups:')\n",
    "print(f'  cycle:   1')\n",
    "print(f'  sensors: {len(SENSOR_COLS)}')\n",
    "print(f'  rolling: {len(ROLL_COLS)}')\n",
    "print(f'  delta:   {len(DELTA_COLS)}')\n",
    "print(f'  RT geom: {len(RT_COLS)}')\n",
    "print(f'  ORTHON:  {len(ORTHON_COLS)}')\n",
    "print(f'  TOTAL:   {len(ALL_FEAT_COLS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5b4a6",
   "metadata": {},
   "source": [
    "## 8. Prepare Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5ef72a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:33:02.741359Z",
     "iopub.status.busy": "2026-02-25T23:33:02.741301Z",
     "iopub.status.idle": "2026-02-25T23:33:03.315867Z",
     "shell.execute_reply": "2026-02-25T23:33:03.315519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing train matrix...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X_train: (20631, 467)\n",
      "  X_test:  (100, 467) (100 engines)\n"
     ]
    }
   ],
   "source": [
    "def prepare_matrix(df, feat_cols, rul_col='RUL', imputer=None, fit_imputer=True):\n",
    "    \"\"\"Extract X, y from DataFrame. Handles inf/nan. Returns (X, y, groups, imputer).\"\"\"\n",
    "    X = df[feat_cols].values.astype(np.float64)\n",
    "    X = np.where(np.isinf(X), np.nan, X)\n",
    "    if fit_imputer:\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X = imputer.fit_transform(X)\n",
    "    else:\n",
    "        X = imputer.transform(X)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y = df[rul_col].values.astype(np.float64) if rul_col in df.columns else None\n",
    "    groups = df['cohort'].values\n",
    "    return X, y, groups, imputer\n",
    "\n",
    "print('Preparing train matrix...')\n",
    "X_train, y_train, groups_train, imputer = prepare_matrix(train_full, ALL_FEAT_COLS)\n",
    "print(f'  X_train: {X_train.shape}')\n",
    "\n",
    "# Test: get last cycle per engine (prediction point)\n",
    "test_cohorts_sorted = sorted(test_full['cohort'].unique(),\n",
    "                              key=lambda x: int(x.split('_')[-1]))\n",
    "last_idx = [test_full[test_full['cohort'] == c]['cycle'].idxmax() for c in test_cohorts_sorted]\n",
    "test_last = test_full.loc[last_idx].reset_index(drop=True)\n",
    "X_test, _, _, _ = prepare_matrix(test_last, ALL_FEAT_COLS, rul_col='RUL',\n",
    "                                  imputer=imputer, fit_imputer=False)\n",
    "y_test = np.clip(rul_gt, 0, RUL_CAP)\n",
    "print(f'  X_test:  {X_test.shape} ({len(test_cohorts_sorted)} engines)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18cef1",
   "metadata": {},
   "source": [
    "## 9. Run A — Standard Models (Ridge / RF / GB)\n",
    "\n",
    "Same architecture as v2 for apple-to-apple comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96697f05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:33:03.317154Z",
     "iopub.status.busy": "2026-02-25T23:33:03.317080Z",
     "iopub.status.idle": "2026-02-25T23:33:04.760830Z",
     "shell.execute_reply": "2026-02-25T23:33:04.760450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run A — Ridge (OOF + Test) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ridge                     [OOF]  RMSE=15.08  MAE=11.09  R²=0.8690  PHM=76,991  bias=-0.43\n",
      "  ridge                     [TEST]  RMSE=17.12  MAE=13.85  R²=0.8174  PHM=432  bias=-0.49\n"
     ]
    }
   ],
   "source": [
    "def phm08(pred, true):\n",
    "    d = pred - true\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def evaluate(name, y_true, y_pred, label=''):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    phm  = phm08(y_pred, y_true)\n",
    "    bias = np.mean(y_pred - y_true)\n",
    "    print(f'  {name:25s} {label}  RMSE={rmse:.2f}  MAE={mae:.2f}  R²={r2:.4f}  PHM={phm:,.0f}  bias={bias:+.2f}')\n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2, 'phm08': phm, 'bias': bias}\n",
    "\n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "scaler_A = StandardScaler()\n",
    "X_train_s = scaler_A.fit_transform(X_train)\n",
    "X_test_s  = scaler_A.transform(X_test)\n",
    "\n",
    "# Run A: Ridge only (fast comparison with v2 Ridge=25.75)\n",
    "resultsA_cv   = {}\n",
    "resultsA_test = {}\n",
    "print('=== Run A — Ridge (OOF + Test) ===')\n",
    "ridge = Ridge(alpha=1.0)\n",
    "oof_r = np.clip(cross_val_predict(ridge, X_train_s, y_train, groups=groups_train, cv=gkf, n_jobs=-1), 0, RUL_CAP)\n",
    "resultsA_cv['ridge'] = evaluate('ridge', y_train, oof_r, '[OOF]')\n",
    "ridge.fit(X_train_s, y_train)\n",
    "pred_r = np.clip(ridge.predict(X_test_s), 0, RUL_CAP)\n",
    "resultsA_test['ridge'] = evaluate('ridge', y_test, pred_r, '[TEST]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6a2be",
   "metadata": {},
   "source": [
    "## 10. Run B — Stacking Ensemble (LGB + XGB + Hist → RidgeCV)\n",
    "\n",
    "Matches the architecture that produced the 11.72 benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe96080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:33:04.762121Z",
     "iopub.status.busy": "2026-02-25T23:33:04.762048Z",
     "iopub.status.idle": "2026-02-25T23:34:03.068393Z",
     "shell.execute_reply": "2026-02-25T23:34:03.067875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating OOF predictions for stacking meta-features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run B — OOF (individual base learners) ===\n",
      "  lgb                       [OOF]  RMSE=13.86  MAE=9.32  R²=0.8893  PHM=78,324  bias=-0.78\n",
      "  xgb                       [OOF]  RMSE=13.79  MAE=9.24  R²=0.8904  PHM=78,459  bias=-0.77\n",
      "  hist                      [OOF]  RMSE=13.92  MAE=9.24  R²=0.8885  PHM=84,713  bias=-0.60\n",
      "\n",
      "Meta RidgeCV selected alpha: 100.0\n",
      "Meta weights: LGB=0.313, XGB=0.472, Hist=0.218\n",
      "  stacking                  [OOF]  RMSE=13.70  MAE=9.03  R²=0.8920  PHM=80,711  bias=-0.07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': np.float64(13.697541467451435),\n",
       " 'mae': 9.027777065344507,\n",
       " 'r2': 0.8919605907854676,\n",
       " 'phm08': 80710.6356413961,\n",
       " 'bias': np.float64(-0.07438422582058062)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base learners\n",
    "lgb_model  = LGBMRegressor(n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "                            num_leaves=63, subsample=0.8, colsample_bytree=0.8,\n",
    "                            min_child_samples=20, random_state=SEED, n_jobs=-1,\n",
    "                            verbose=-1)\n",
    "xgb_model  = XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "                           subsample=0.8, colsample_bytree=0.8,\n",
    "                           min_child_weight=5, random_state=SEED, n_jobs=-1,\n",
    "                           verbosity=0)\n",
    "hist_model = HistGradientBoostingRegressor(max_iter=500, max_depth=6, learning_rate=0.05,\n",
    "                                            min_samples_leaf=20, random_state=SEED)\n",
    "\n",
    "# Generate OOF predictions — use n_estimators=300 for speed, 500 for final models\n",
    "print('Generating OOF predictions for stacking meta-features...')\n",
    "lgb_oof_m  = LGBMRegressor(n_estimators=300, max_depth=6, learning_rate=0.05, num_leaves=63, subsample=0.8, colsample_bytree=0.8, min_child_samples=20, random_state=SEED, n_jobs=-1, verbose=-1)\n",
    "xgb_oof_m  = XGBRegressor(n_estimators=300, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, min_child_weight=5, random_state=SEED, n_jobs=-1, verbosity=0)\n",
    "hist_oof_m = HistGradientBoostingRegressor(max_iter=300, max_depth=6, learning_rate=0.05, min_samples_leaf=20, random_state=SEED)\n",
    "lgb_oof  = cross_val_predict(lgb_oof_m,  X_train, y_train, groups=groups_train, cv=gkf, n_jobs=1)\n",
    "xgb_oof  = cross_val_predict(xgb_oof_m,  X_train, y_train, groups=groups_train, cv=gkf, n_jobs=1)\n",
    "hist_oof = cross_val_predict(hist_oof_m, X_train, y_train, groups=groups_train, cv=gkf, n_jobs=1)\n",
    "\n",
    "lgb_oof  = np.clip(lgb_oof,  0, RUL_CAP)\n",
    "xgb_oof  = np.clip(xgb_oof,  0, RUL_CAP)\n",
    "hist_oof = np.clip(hist_oof, 0, RUL_CAP)\n",
    "\n",
    "print('\\n=== Run B — OOF (individual base learners) ===')\n",
    "evaluate('lgb',  y_train, lgb_oof,  '[OOF]')\n",
    "evaluate('xgb',  y_train, xgb_oof,  '[OOF]')\n",
    "evaluate('hist', y_train, hist_oof, '[OOF]')\n",
    "\n",
    "# Meta-learner on OOF\n",
    "meta_X_oof = np.column_stack([lgb_oof, xgb_oof, hist_oof])\n",
    "meta = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0, 100.0])\n",
    "meta.fit(meta_X_oof, y_train)\n",
    "stack_oof = np.clip(meta.predict(meta_X_oof), 0, RUL_CAP)\n",
    "oof_gap = np.sqrt(mean_squared_error(y_train, stack_oof))\n",
    "\n",
    "print(f'\\nMeta RidgeCV selected alpha: {meta.alpha_}')\n",
    "print(f'Meta weights: LGB={meta.coef_[0]:.3f}, XGB={meta.coef_[1]:.3f}, Hist={meta.coef_[2]:.3f}')\n",
    "evaluate('stacking', y_train, stack_oof, '[OOF]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5659697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:34:03.070521Z",
     "iopub.status.busy": "2026-02-25T23:34:03.070391Z",
     "iopub.status.idle": "2026-02-25T23:34:25.220886Z",
     "shell.execute_reply": "2026-02-25T23:34:25.220288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final base learners on full train set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run B — Test (individual base learners) ===\n",
      "  lgb                       [TEST]  RMSE=16.29  MAE=11.45  R²=0.8347  PHM=411  bias=-2.70\n",
      "  xgb                       [TEST]  RMSE=17.02  MAE=11.96  R²=0.8197  PHM=460  bias=-2.55\n",
      "  hist                      [TEST]  RMSE=17.13  MAE=12.04  R²=0.8173  PHM=472  bias=-2.35\n",
      "  stacking                  [TEST]  RMSE=16.60  MAE=11.59  R²=0.8284  PHM=440  bias=-1.87\n",
      "\n",
      "OOF RMSE: 13.70  Test RMSE: 16.60  Gap: +2.90\n"
     ]
    }
   ],
   "source": [
    "# Train final base learners on full train set\n",
    "print('Training final base learners on full train set...')\n",
    "lgb_final  = lgb_model.__class__(**lgb_model.get_params()).fit(X_train,  y_train)\n",
    "xgb_final  = xgb_model.__class__(**xgb_model.get_params()).fit(X_train,  y_train)\n",
    "hist_final = hist_model.__class__(**hist_model.get_params()).fit(X_train, y_train)\n",
    "\n",
    "# Test predictions\n",
    "lgb_test  = np.clip(lgb_final.predict(X_test),   0, RUL_CAP)\n",
    "xgb_test  = np.clip(xgb_final.predict(X_test),   0, RUL_CAP)\n",
    "hist_test = np.clip(hist_final.predict(X_test),   0, RUL_CAP)\n",
    "meta_test = np.column_stack([lgb_test, xgb_test, hist_test])\n",
    "stack_test = np.clip(meta.predict(meta_test), 0, RUL_CAP)\n",
    "\n",
    "print('\\n=== Run B — Test (individual base learners) ===')\n",
    "resultsB_test = {}\n",
    "resultsB_test['lgb']  = evaluate('lgb',      y_test, lgb_test,   '[TEST]')\n",
    "resultsB_test['xgb']  = evaluate('xgb',      y_test, xgb_test,   '[TEST]')\n",
    "resultsB_test['hist'] = evaluate('hist',      y_test, hist_test,  '[TEST]')\n",
    "resultsB_test['stacking'] = evaluate('stacking', y_test, stack_test, '[TEST]')\n",
    "\n",
    "test_rmse = resultsB_test['stacking']['rmse']\n",
    "oof_rmse  = oof_gap\n",
    "gap       = test_rmse - oof_rmse\n",
    "print(f'\\nOOF RMSE: {oof_rmse:.2f}  Test RMSE: {test_rmse:.2f}  Gap: {gap:+.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01619f",
   "metadata": {},
   "source": [
    "## 11. Feature Importance (LGB on full train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83cc4653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:34:25.222453Z",
     "iopub.status.busy": "2026-02-25T23:34:25.222376Z",
     "iopub.status.idle": "2026-02-25T23:34:25.227154Z",
     "shell.execute_reply": "2026-02-25T23:34:25.226529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank                                                  Feature  Importance     Layer\n",
      "----------------------------------------------------------------------------------\n",
      "   1                                                    cycle         819     cycle\n",
      "   2                                   ed_condition_number_d1         461    eigenD\n",
      "   3                                          ed_ratio_2_1_d2         309    eigenD\n",
      "   4                                          ed_ratio_2_1_d1         270    eigenD\n",
      "   5                      ed_eigenvalue_entropy_normalized_d2         233    eigenD\n",
      "   6                                      ed_condition_number         232    eigenD\n",
      "   7                                        ed_total_variance         230    eigenD\n",
      "   8                                         rt_centroid_dist         224   RT_geom\n",
      "   9                                     ed_total_variance_d2         220    eigenD\n",
      "  10                      ed_eigenvalue_entropy_normalized_d1         185    eigenD\n",
      "  11                                   ed_condition_number_d2         158    eigenD\n",
      "  12                                 ed_eigenvalue_entropy_d2         156    eigenD\n",
      "  13                                         roll_Nc_range_30         139   rolling\n",
      "  14                                          roll_P30_std_30         136   rolling\n",
      "  15                                          ed_eigenvalue_0         134    eigenD\n",
      "  16                                          ed_eigenvalue_3         131    eigenD\n",
      "  17                                             ed_ratio_2_1         130    eigenD\n",
      "  18                                          roll_W32_max_30         130   rolling\n",
      "  19                                     ed_explained_ratio_3         122    eigenD\n",
      "  20                                        roll_T50_range_30         122   rolling\n",
      "  21                                        roll_P30_range_30         121   rolling\n",
      "  22                                             ed_ratio_3_1         119    eigenD\n",
      "  23                                       roll_Ps30_range_30         119   rolling\n",
      "  24                                         roll_Ps30_std_30         116   rolling\n",
      "  25                                          ed_eigenvalue_7         114    eigenD\n",
      "  26                                        roll_T24_range_30         113   rolling\n",
      "  27                                           roll_Nc_std_30         112   rolling\n",
      "  28                                          roll_phi_std_30         111   rolling\n",
      "  29                                          roll_T24_max_30         110   rolling\n",
      "  30                                        roll_T30_range_30         110   rolling\n",
      "feat_names: 604, importances: 467\n",
      "\n",
      "=== Layer importance (total) ===\n",
      "  cycle       :      819  (3.7%)\n",
      "  sensors     :      252  (1.1%)\n",
      "  rolling     :    14710  (66.7%)\n",
      "  delta       :      359  (1.6%)\n",
      "  RT_geom     :      399  (1.8%)\n",
      "  ORTHON      :     5511  (25.0%)\n"
     ]
    }
   ],
   "source": [
    "importances = lgb_final.feature_importances_\n",
    "feat_names  = np.array(ALL_FEAT_COLS)\n",
    "order = np.argsort(importances)[::-1]\n",
    "\n",
    "print(f'{\"Rank\":>4s}  {\"Feature\":>55s}  {\"Importance\":>10s}  {\"Layer\":>8s}')\n",
    "print('-' * 82)\n",
    "for rank, i in enumerate(order[:30]):\n",
    "    name = feat_names[i]\n",
    "    if name == 'cycle':              layer = 'cycle'\n",
    "    elif name in SENSORS_15:         layer = 'sensor'\n",
    "    elif name.startswith('roll_'):   layer = 'rolling'\n",
    "    elif name.startswith('delta_'):  layer = 'delta'\n",
    "    elif name.startswith('rt_'):     layer = 'RT_geom'\n",
    "    elif name.startswith('ed_'):     layer = 'eigenD'\n",
    "    elif name.startswith('ph_'):     layer = 'homology'\n",
    "    elif name.startswith('cv_'):     layer = 'centroid'\n",
    "    elif name.startswith('prim_'):   layer = 'primitives'\n",
    "    elif name.startswith('traj_'):   layer = 'traj'\n",
    "    else:                            layer = '?'\n",
    "    print(f'{rank+1:>4d}  {name:>55s}  {importances[i]:>10.0f}  {layer:>8s}')\n",
    "\n",
    "print(f'feat_names: {len(feat_names)}, importances: {len(importances)}')\n",
    "# Sync feat_names to importances length in case of internal LGB deduplication\n",
    "feat_names_trim = feat_names[:len(importances)]\n",
    "\n",
    "# Layer-level total importance\n",
    "print('\\n=== Layer importance (total) ===')\n",
    "layers = {\n",
    "    'cycle':    ['cycle'],\n",
    "    'sensors':  SENSORS_15,\n",
    "    'rolling':  ROLL_COLS,\n",
    "    'delta':    DELTA_COLS,\n",
    "    'RT_geom':  RT_COLS,\n",
    "    'ORTHON':   ORTHON_COLS,\n",
    "}\n",
    "total_imp = importances.sum()\n",
    "for layer_name, cols in layers.items():\n",
    "    cols_set = set(cols)\n",
    "    idx = [i for i, c in enumerate(feat_names_trim) if c in cols_set]\n",
    "    layer_imp = importances[idx].sum() if idx else 0\n",
    "    print(f'  {layer_name:12s}: {layer_imp:8.0f}  ({layer_imp/total_imp:.1%})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b15bf",
   "metadata": {},
   "source": [
    "## 12. Ablation — What Does Each Layer Add?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7eb9675",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:34:25.228687Z",
     "iopub.status.busy": "2026-02-25T23:34:25.228585Z",
     "iopub.status.idle": "2026-02-25T23:35:51.418571Z",
     "shell.execute_reply": "2026-02-25T23:35:51.418106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration                                      OOF     Test      Gap\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cycle + sensors                                  16.73    17.45    +0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ rolling (CSV baseline)                         15.30    16.36    +1.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ RT geometry                                    15.42    15.15    -0.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ ORTHON only (no RT)                            13.84    17.36    +3.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 4: + RT + ORTHON                          13.80    16.48    +2.68\n"
     ]
    }
   ],
   "source": [
    "def quick_lgb(X_tr, y_tr, X_te, y_te, groups, gkf, rul_cap=125):\n",
    "    \"\"\"Quick LGB OOF + test eval.\"\"\"\n",
    "    model = LGBMRegressor(n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "                          random_state=SEED, n_jobs=-1, verbose=-1)\n",
    "    oof = np.clip(cross_val_predict(model, X_tr, y_tr, groups=groups, cv=gkf, n_jobs=1), 0, rul_cap)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    test_pred = np.clip(model.predict(X_te), 0, rul_cap)\n",
    "    oof_rmse  = np.sqrt(mean_squared_error(y_tr, oof))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_te, test_pred))\n",
    "    return oof_rmse, test_rmse, test_rmse - oof_rmse\n",
    "\n",
    "ablation_sets = [\n",
    "    ('cycle + sensors',                  ['cycle'] + SENSORS_15),\n",
    "    ('+ rolling (CSV baseline)',          ['cycle'] + SENSORS_15 + ROLL_COLS + DELTA_COLS),\n",
    "    ('+ RT geometry',                     ['cycle'] + SENSORS_15 + ROLL_COLS + DELTA_COLS + RT_COLS),\n",
    "    ('+ ORTHON only (no RT)',             ['cycle'] + SENSORS_15 + ROLL_COLS + DELTA_COLS + ORTHON_COLS),\n",
    "    ('Config 4: + RT + ORTHON',           ALL_FEAT_COLS),\n",
    "]\n",
    "\n",
    "print(f'{\"Configuration\":45s}  {\"OOF\":>7s}  {\"Test\":>7s}  {\"Gap\":>7s}')\n",
    "print('-' * 70)\n",
    "ablation_results = {}\n",
    "for label, cols in ablation_sets:\n",
    "    cols_avail = [c for c in cols if c in train_full.columns]\n",
    "    X_tr_abl, y_tr_abl, grps, imp_abl = prepare_matrix(train_full, cols_avail)\n",
    "    X_te_abl, _, _, _                  = prepare_matrix(test_last, cols_avail, imputer=imp_abl, fit_imputer=False)\n",
    "    oof_r, tst_r, gap_r = quick_lgb(X_tr_abl, y_train, X_te_abl, y_test, groups_train, gkf)\n",
    "    print(f'{label:45s}  {oof_r:7.2f}  {tst_r:7.2f}  {gap_r:+7.2f}')\n",
    "    ablation_results[label] = {'oof_rmse': oof_r, 'test_rmse': tst_r, 'gap': gap_r}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5813d",
   "metadata": {},
   "source": [
    "## 13. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "606d032a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:35:51.419650Z",
     "iopub.status.busy": "2026-02-25T23:35:51.419577Z",
     "iopub.status.idle": "2026-02-25T23:35:51.422771Z",
     "shell.execute_reply": "2026-02-25T23:35:51.422466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RESULTS SUMMARY — FD001 RUL Prediction\n",
      "================================================================================\n",
      "Config                                       RMSE      PHM08      Gap  Features\n",
      "--------------------------------------------------------------------------------\n",
      "CSV Standalone (prior)                      12.16        224    +1.05  ~1,044\n",
      "ORTHON Alone v2 (prior)                     16.31        382    +1.07  14\n",
      "ORTHON+CSV v3 window-level (prior)          15.03        374    +0.92  38\n",
      "RT Geometry baseline (prior 11.72)          11.72        188    -0.70  413\n",
      "--------------------------------------------------------------------------------\n",
      "Config 4 Run A (ridge)                      17.12        432    -0.49  604\n",
      "Config 4 Run B (stacking)                   16.60        440    +2.90  604\n",
      "================================================================================\n",
      "\n",
      "Config 4 stacking vs 11.72 benchmark: +4.88 RMSE (missed by 4.88)\n"
     ]
    }
   ],
   "source": [
    "print('=' * 80)\n",
    "print('RESULTS SUMMARY — FD001 RUL Prediction')\n",
    "print('=' * 80)\n",
    "print(f'{\"Config\":40s}  {\"RMSE\":>7s}  {\"PHM08\":>9s}  {\"Gap\":>7s}  {\"Features\"}')\n",
    "print('-' * 80)\n",
    "\n",
    "benchmarks = [\n",
    "    ('CSV Standalone (prior)',             12.16, 224,  '+1.05', '~1,044'),\n",
    "    ('ORTHON Alone v2 (prior)',            16.31, 382,  '+1.07', '14'),\n",
    "    ('ORTHON+CSV v3 window-level (prior)', 15.03, 374,  '+0.92', '38'),\n",
    "    ('RT Geometry baseline (prior 11.72)', 11.72, 188,  '-0.70', '413'),\n",
    "]\n",
    "for name, rmse, phm, gap, feats in benchmarks:\n",
    "    print(f'{name:40s}  {rmse:7.2f}  {phm:9,}  {gap:>7s}  {feats}')\n",
    "\n",
    "print('-' * 80)\n",
    "# Run A best\n",
    "best_A = min(resultsA_test, key=lambda k: resultsA_test[k]['rmse'])\n",
    "r = resultsA_test[best_A]\n",
    "gap_A = r['rmse'] - min(resultsA_cv[best_A]['rmse'], r['rmse'])  # approximate\n",
    "print(f'{\"Config 4 Run A (\" + best_A + \")\": <40s}  {r[\"rmse\"]:7.2f}  {r[\"phm08\"]:9,.0f}  {r[\"bias\"]:+7.2f}  {len(ALL_FEAT_COLS)}')\n",
    "\n",
    "# Run B stacking\n",
    "r = resultsB_test['stacking']\n",
    "gap_B = test_rmse - oof_rmse\n",
    "print(f'{\"Config 4 Run B (stacking)\":40s}  {r[\"rmse\"]:7.2f}  {r[\"phm08\"]:9,.0f}  {gap_B:+7.2f}  {len(ALL_FEAT_COLS)}')\n",
    "print('=' * 80)\n",
    "\n",
    "best_rmse = r['rmse']\n",
    "vs_benchmark = best_rmse - 11.72\n",
    "print(f'\\nConfig 4 stacking vs 11.72 benchmark: {vs_benchmark:+.2f} RMSE ({\"BEAT\" if vs_benchmark < 0 else \"missed by\"} {abs(vs_benchmark):.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db19bbb",
   "metadata": {},
   "source": [
    "## 14. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "874d2e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T23:35:51.423974Z",
     "iopub.status.busy": "2026-02-25T23:35:51.423913Z",
     "iopub.status.idle": "2026-02-25T23:35:51.427409Z",
     "shell.execute_reply": "2026-02-25T23:35:51.427133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /Users/jasonrudder/domains/cmapss/FD_001/Train/output_time/ml_results_config4/summary.json\n",
      "{\n",
      "  \"experiment\": \"config4_per_cycle_rt_orthon\",\n",
      "  \"dataset\": \"FD001\",\n",
      "  \"n_train_rows\": 20631,\n",
      "  \"n_features\": 467,\n",
      "  \"feature_groups\": {\n",
      "    \"cycle\": 1,\n",
      "    \"sensors\": 15,\n",
      "    \"rolling\": 375,\n",
      "    \"delta\": 15,\n",
      "    \"rt_geometry\": 5,\n",
      "    \"orthon\": 193\n",
      "  },\n",
      "  \"fleet_baseline\": {\n",
      "    \"early_life_cycles\": 30,\n",
      "    \"n_pca_components\": 10,\n",
      "    \"pc1_explained\": 0.482018984132283\n",
      "  },\n",
      "  \"run_a_test\": {\n",
      "    \"ridge\": {\n",
      "      \"rmse\": 17.123635422213194,\n",
      "      \"mae\": 13.85078847032697,\n",
      "      \"r2\": 0.8174077935615021,\n",
      "      \"phm08\": 432.27172593579684,\n",
      "      \"bias\": -0.4859616346492115\n",
      "    }\n",
      "  },\n",
      "  \"run_b_test\": {\n",
      "    \"lgb\": {\n",
      "      \"rmse\": 16.2924759507697,\n",
      "      \"mae\": 11.452509543046444,\n",
      "      \"r2\": 0.8347031915108756,\n",
      "      \"phm08\": 411.04611077442473,\n",
      "      \"bias\": -2.6957856208428486\n",
      "    },\n",
      "    \"xgb\": {\n",
      "      \"rmse\": 17.016990136501455,\n",
      "      \"mae\": 11.963552231788634,\n",
      "      \"r2\": 0.819675064533165,\n",
      "      \"phm08\": 459.6757552728687,\n",
      "      \"bias\": -2.551956753730774\n",
      "    },\n",
      "    \"hist\": {\n",
      "      \"rmse\": 17.12828677518,\n",
      "      \"mae\": 12.03691432049262,\n",
      "      \"r2\": 0.8173085837699523,\n",
      "      \"phm08\": 472.44577924288774,\n",
      "      \"bias\": -2.354320216224596\n",
      "    },\n",
      "    \"stacking\": {\n",
      "      \"rmse\": 16.598340668085818,\n",
      "      \"mae\": 11.594835307451637,\n",
      "      \"r2\": 0.8284385773210856,\n",
      "      \"phm08\": 440.1272660788667,\n",
      "      \"bias\": -1.8651575617580949\n",
      "    }\n",
      "  },\n",
      "  \"run_b_oof_rmse\": 13.697541467451435,\n",
      "  \"run_b_gap\": 2.900799200634383,\n",
      "  \"benchmark_11_72\": 11.72,\n",
      "  \"delta_vs_benchmark\": 4.8783406680858175,\n",
      "  \"beat_benchmark\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "out_dir = TRAIN_BASE / 'output_time/ml_results_config4'\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'config4_per_cycle_rt_orthon',\n",
    "    'dataset':    'FD001',\n",
    "    'n_train_rows': int(X_train.shape[0]),\n",
    "    'n_features': int(X_train.shape[1]),\n",
    "    'feature_groups': {\n",
    "        'cycle': 1,\n",
    "        'sensors': len(SENSOR_COLS),\n",
    "        'rolling': len(ROLL_COLS),\n",
    "        'delta': len(DELTA_COLS),\n",
    "        'rt_geometry': len(RT_COLS),\n",
    "        'orthon': len(ORTHON_COLS),\n",
    "    },\n",
    "    'fleet_baseline': {\n",
    "        'early_life_cycles': EARLY_LIFE,\n",
    "        'n_pca_components': N_PCA,\n",
    "        'pc1_explained': float(fleet_pca.explained_variance_ratio_[0]),\n",
    "    },\n",
    "    'run_a_test': resultsA_test,\n",
    "    'run_b_test': resultsB_test,\n",
    "    'run_b_oof_rmse': float(oof_rmse),\n",
    "    'run_b_gap': float(gap_B),\n",
    "    'ablation': ablation_results,\n",
    "    'benchmark_11_72': 11.72,\n",
    "    'delta_vs_benchmark': float(vs_benchmark),\n",
    "    'beat_benchmark': bool(vs_benchmark < 0),\n",
    "}\n",
    "\n",
    "with open(out_dir / 'summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f'Saved to {out_dir}/summary.json')\n",
    "print(json.dumps({k: v for k, v in summary.items() if k not in ['ablation']}, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
