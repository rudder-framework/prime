{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FD001 RUL Prediction — Experiment v2\n",
    "\n",
    "**Change from v1:** `compute_derivatives()` in `packages/geometry/src/geometry/dynamics.py` was non-causal.\n",
    "Central differences + symmetric smoothing meant `effective_dim_velocity[t]` used windows `t+1` and `t+2`.\n",
    "\n",
    "**v2 fix:**\n",
    "- Backward-only smoothing (cumsum-based causal moving average)\n",
    "- Backward finite differences (velocity at t uses only t and t-1)\n",
    "- Also fixed `smooth_window` passthrough bug: orchestration pipeline now reads `derivative_depth` from manifest\n",
    "\n",
    "**Parquets regenerated:** FD001 Train (220s) and Test (153s) re-run with causal derivatives.\n",
    "\n",
    "**Hypothesis:** Geodyn features (effective_dim_velocity, spectral_gap_velocity, etc.) should transfer better\n",
    "across cohorts now that they don't encode future information. Centroid features (cv_*) should be unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = Path('/Users/jasonrudder/domains/cmapss/FD_001/Train/output_time')\n",
    "TEST_DIR  = Path('/Users/jasonrudder/domains/cmapss/FD_001/Test/output_time')\n",
    "RUL_PATH  = Path('/Users/jasonrudder/domains/cmapss/FD_001/RUL_FD001.txt')\n",
    "OBS_TRAIN = TRAIN_DIR / 'observations.parquet'\n",
    "OBS_TEST  = TEST_DIR / 'observations.parquet'\n",
    "\n",
    "EXPERIMENT = 'v2_causal_derivatives'\n",
    "RUL_CAP = 125\n",
    "N_FOLDS = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(output_dir: Path, obs_path: Path) -> pl.DataFrame:\n",
    "    \"\"\"Build ML feature matrix from pipeline parquets.\"\"\"\n",
    "    # Cohort vector (base)\n",
    "    cv = pl.read_parquet(str(output_dir / 'cohort/cohort_vector.parquet'))\n",
    "    join_cols = ['cohort', 'signal_0_center']\n",
    "    feat_cols = [c for c in cv.columns if c not in ['signal_0_start', 'signal_0_end', 'signal_0_center', 'cohort', 'n_signals']]\n",
    "    ml = cv.select(['cohort', 'signal_0_center'] + feat_cols).rename({c: f'cv_{c}' for c in feat_cols})\n",
    "    print(f'  cohort_vector: {len(feat_cols)} features, {len(ml)} rows')\n",
    "\n",
    "    # Geometry dynamics\n",
    "    gd_path = output_dir / 'cohort/cohort_dynamics/geometry_dynamics.parquet'\n",
    "    if gd_path.exists():\n",
    "        gd = pl.read_parquet(str(gd_path))\n",
    "        gd_feat = [c for c in gd.columns if c not in ['cohort', 'signal_0_start', 'signal_0_end', 'signal_0_center', 'I']]\n",
    "        gd_sel = gd.select(['cohort', 'signal_0_center'] + gd_feat).rename({c: f'gd_{c}' for c in gd_feat})\n",
    "        before = ml.shape[1]\n",
    "        ml = ml.join(gd_sel, on=join_cols, how='left', coalesce=True)\n",
    "        print(f'  + geometry_dynamics: {ml.shape[1] - before} features')\n",
    "\n",
    "    # Velocity field (aggregate per cohort+window since it's per-signal)\n",
    "    vf_path = output_dir / 'cohort/cohort_dynamics/velocity_field.parquet'\n",
    "    if vf_path.exists():\n",
    "        vf = pl.read_parquet(str(vf_path))\n",
    "        num_cols = [c for c in vf.columns if c not in ['cohort', 'signal_0_start', 'signal_0_end', 'signal_0_center',\n",
    "                    'signal_id', 'dominant_motion_signal'] and vf[c].dtype in [pl.Float64, pl.Float32, pl.Int64]]\n",
    "        if 'signal_0_center' in vf.columns and num_cols:\n",
    "            vf_agg = vf.group_by(['cohort', 'signal_0_center']).agg(\n",
    "                [pl.col(c).mean().alias(f'vf_{c}_mean') for c in num_cols] +\n",
    "                [pl.col(c).std().alias(f'vf_{c}_std') for c in num_cols]\n",
    "            )\n",
    "            before = ml.shape[1]\n",
    "            ml = ml.join(vf_agg, on=join_cols, how='left', coalesce=True)\n",
    "            print(f'  + velocity_field: {ml.shape[1] - before} features')\n",
    "\n",
    "    # Cohort geometry\n",
    "    cg_path = output_dir / 'cohort/cohort_geometry.parquet'\n",
    "    if cg_path.exists():\n",
    "        cg = pl.read_parquet(str(cg_path))\n",
    "        cg_feat = [c for c in cg.columns if c not in ['cohort', 'signal_0_start', 'signal_0_end', 'signal_0_center',\n",
    "                    'window_index', 'signal_id'] and cg[c].dtype in [pl.Float64, pl.Float32]]\n",
    "        if 'signal_0_center' in cg.columns and cg_feat:\n",
    "            cg_agg = cg.group_by(['cohort', 'signal_0_center']).agg(\n",
    "                [pl.col(c).mean().alias(f'cg_{c}_mean') for c in cg_feat]\n",
    "            )\n",
    "            before = ml.shape[1]\n",
    "            ml = ml.join(cg_agg, on=join_cols, how='left', coalesce=True)\n",
    "            print(f'  + cohort_geometry: {ml.shape[1] - before} features')\n",
    "\n",
    "    # Add RUL\n",
    "    obs = pl.read_parquet(str(obs_path))\n",
    "    first_sig = obs['signal_id'].unique().sort()[0]\n",
    "    lifecycles = dict(\n",
    "        obs.filter(pl.col('signal_id') == first_sig)\n",
    "        .group_by('cohort').agg(pl.col('signal_0').max().alias('max_s0'))\n",
    "        .iter_rows()\n",
    "    )\n",
    "\n",
    "    rul_rows = []\n",
    "    for row in ml.select(['cohort', 'signal_0_center']).iter_rows(named=True):\n",
    "        max_s0 = lifecycles.get(row['cohort'])\n",
    "        if max_s0 is not None:\n",
    "            rul_rows.append({\n",
    "                'cohort': row['cohort'],\n",
    "                'signal_0_center': row['signal_0_center'],\n",
    "                'RUL': max_s0 - row['signal_0_center'],\n",
    "                'lifecycle': max_s0,\n",
    "                'lifecycle_pct': row['signal_0_center'] / max_s0 if max_s0 > 0 else 0,\n",
    "            })\n",
    "\n",
    "    rul_df = pl.DataFrame(rul_rows)\n",
    "    ml = ml.join(rul_df, on=['cohort', 'signal_0_center'], how='left', coalesce=True)\n",
    "\n",
    "    # Drop constant columns\n",
    "    feat_cols = [c for c in ml.columns if c not in ['cohort', 'signal_0_center', 'RUL', 'lifecycle', 'lifecycle_pct']]\n",
    "    drop_const = [c for c in feat_cols if ml[c].dtype in [pl.Float64, pl.Float32] and\n",
    "                  (ml[c].drop_nulls().std() or 0) < 1e-10]\n",
    "    if drop_const:\n",
    "        ml = ml.drop(drop_const)\n",
    "        print(f'  Dropped {len(drop_const)} constant columns')\n",
    "\n",
    "    feat_final = [c for c in ml.columns if c not in ['cohort', 'signal_0_center', 'RUL', 'lifecycle', 'lifecycle_pct']]\n",
    "    print(f'  Total: {len(feat_final)} features, {len(ml)} rows, {ml[\"cohort\"].n_unique()} cohorts')\n",
    "    return ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== TRAIN ===')\n",
    "ml_train = build_features(TRAIN_DIR, OBS_TRAIN)\n",
    "print(f'\\n=== TEST ===')\n",
    "ml_test = build_features(TEST_DIR, OBS_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Validation on Train (GroupKFold by cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "META = ['cohort', 'signal_0_center', 'RUL', 'lifecycle', 'lifecycle_pct']\n\ndef prepare(ml, cap=RUL_CAP):\n    feat_cols = [c for c in ml.columns if c not in META]\n    X = ml.select(feat_cols).to_numpy().astype(np.float64)\n    y = np.minimum(ml['RUL'].to_numpy().astype(np.float64), cap)\n    groups = ml['cohort'].to_numpy()\n    # Replace inf BEFORE imputation (SimpleImputer chokes on inf)\n    X = np.where(np.isinf(X), np.nan, X)\n    imp = SimpleImputer(strategy='median')\n    X = imp.fit_transform(X)\n    # Belt-and-suspenders: catch any remaining non-finite\n    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n    return X, y, groups, feat_cols, imp\n\nX_train, y_train, groups_train, feat_names, imputer = prepare(ml_train)\nprint(f'Train: {X_train.shape[0]} rows x {X_train.shape[1]} features')\nprint(f'RUL range (capped): {y_train.min():.0f} – {y_train.max():.0f}')\nprint(f'Any inf remaining: {np.any(np.isinf(X_train))}')\nprint(f'Any NaN remaining: {np.any(np.isnan(X_train))}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'ridge': Ridge(alpha=1.0),\n",
    "    'random_forest': RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=10, min_samples_leaf=5,\n",
    "        random_state=SEED, n_jobs=-1),\n",
    "    'gradient_boosting': GradientBoostingRegressor(\n",
    "        n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "        min_samples_leaf=5, subsample=0.8, random_state=SEED),\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "\n",
    "for name, model in models.items():\n",
    "    rmses, maes, r2s = [], [], []\n",
    "    for fold, (tr_idx, te_idx) in enumerate(gkf.split(X_train, y_train, groups_train)):\n",
    "        scaler = StandardScaler()\n",
    "        Xtr = scaler.fit_transform(X_train[tr_idx])\n",
    "        Xte = scaler.transform(X_train[te_idx])\n",
    "        m = model.__class__(**model.get_params())\n",
    "        m.fit(Xtr, y_train[tr_idx])\n",
    "        pred = np.clip(m.predict(Xte), 0, RUL_CAP)\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_train[te_idx], pred)))\n",
    "        maes.append(mean_absolute_error(y_train[te_idx], pred))\n",
    "        r2s.append(r2_score(y_train[te_idx], pred))\n",
    "    cv_results[name] = {\n",
    "        'rmse': np.mean(rmses), 'rmse_std': np.std(rmses),\n",
    "        'mae': np.mean(maes), 'mae_std': np.std(maes),\n",
    "        'r2': np.mean(r2s),\n",
    "    }\n",
    "    print(f'{name:>20s}  RMSE={np.mean(rmses):.2f} ± {np.std(rmses):.2f}  '\n",
    "          f'MAE={np.mean(maes):.2f}  R²={np.mean(r2s):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Official Test Evaluation (Train→Test transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth RUL for test engines\n",
    "rul_gt = np.loadtxt(str(RUL_PATH))\n",
    "rul_gt_capped = np.clip(rul_gt, 0, RUL_CAP)\n",
    "print(f'Ground truth RUL: {len(rul_gt)} engines')\n",
    "\n",
    "# Align test features: use common columns with train\n",
    "test_feat_cols = [c for c in ml_test.columns if c not in META]\n",
    "common_feats = sorted(set(feat_names) & set(test_feat_cols))\n",
    "missing_in_test = sorted(set(feat_names) - set(test_feat_cols))\n",
    "print(f'Common features: {len(common_feats)} / {len(feat_names)}')\n",
    "if missing_in_test:\n",
    "    print(f'Missing in test: {len(missing_in_test)} — will fill with 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build test matrix aligned with train columns\nX_test_full = np.zeros((len(ml_test), len(feat_names)))\nfor i, col in enumerate(feat_names):\n    if col in test_feat_cols:\n        X_test_full[:, i] = ml_test[col].to_numpy().astype(np.float64)\n\n# Replace inf before imputer, then impute, then catch stragglers\nX_test_full = np.where(np.isinf(X_test_full), np.nan, X_test_full)\nX_test_full = imputer.transform(X_test_full)\nX_test_full = np.nan_to_num(X_test_full, nan=0.0, posinf=0.0, neginf=0.0)\n\n# Get last window per test cohort (closest to failure)\ntest_cohorts = ml_test['cohort'].unique().sort().to_list()\nlast_idx = []\nfor coh in test_cohorts:\n    mask = ml_test['cohort'] == coh\n    coh_rows = ml_test.filter(mask)\n    max_s0 = coh_rows['signal_0_center'].max()\n    idx = ml_test.with_row_index('_idx').filter(\n        (pl.col('cohort') == coh) & (pl.col('signal_0_center') == max_s0)\n    )['_idx'][0]\n    last_idx.append(idx)\n\nX_test_last = X_test_full[last_idx]\nprint(f'Test engines: {len(X_test_last)}')\n\n# Map test cohorts to RUL ground truth (engine_1 → index 0, etc.)\ncohort_to_idx = {}\nfor coh in test_cohorts:\n    num = int(coh.replace('engine_', ''))\n    cohort_to_idx[coh] = num - 1  # 1-indexed to 0-indexed\n\ny_test_gt = np.array([rul_gt_capped[cohort_to_idx[c]] for c in test_cohorts])\nprint(f'Ground truth aligned: {len(y_test_gt)} engines')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full train set, predict on test\n",
    "scaler_full = StandardScaler()\n",
    "X_train_scaled = scaler_full.fit_transform(X_train)\n",
    "X_test_scaled = scaler_full.transform(X_test_last)\n",
    "\n",
    "test_results = {}\n",
    "for name, model in models.items():\n",
    "    m = model.__class__(**model.get_params())\n",
    "    m.fit(X_train_scaled, y_train)\n",
    "    pred = np.clip(m.predict(X_test_scaled), 0, RUL_CAP)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_gt, pred))\n",
    "    mae = mean_absolute_error(y_test_gt, pred)\n",
    "    r2 = r2_score(y_test_gt, pred)\n",
    "    \n",
    "    # PHM08 scoring function\n",
    "    d = pred - y_test_gt\n",
    "    phm = np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1))\n",
    "    bias = np.mean(d)\n",
    "    n_early = np.sum(d < 0)\n",
    "    n_late = np.sum(d > 0)\n",
    "    \n",
    "    test_results[name] = {\n",
    "        'rmse': rmse, 'mae': mae, 'r2': r2,\n",
    "        'phm08_score': phm, 'bias': bias,\n",
    "        'n_early': int(n_early), 'n_late': int(n_late),\n",
    "    }\n",
    "    print(f'{name:>20s}  RMSE={rmse:.2f}  MAE={mae:.2f}  R²={r2:.4f}  '\n",
    "          f'PHM08={phm:.0f}  bias={bias:.1f} ({n_early}E/{n_late}L)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GB on full data for importance\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "    min_samples_leaf=5, subsample=0.8, random_state=SEED)\n",
    "gb.fit(X_train_scaled, y_train)\n",
    "\n",
    "imp_idx = np.argsort(gb.feature_importances_)[::-1]\n",
    "print(f'{\"Rank\":>4s}  {\"Feature\":>50s}  {\"Importance\":>10s}  {\"Group\":>5s}')\n",
    "print('-' * 75)\n",
    "for rank, i in enumerate(imp_idx[:25]):\n",
    "    feat = feat_names[i]\n",
    "    group = feat.split('_')[0]\n",
    "    print(f'{rank+1:>4d}  {feat:>50s}  {gb.feature_importances_[i]:>10.4f}  {group:>5s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison with v1 (pre-causal fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1 results (from ~/domains/testing/FD_001/test/output_sequential/ml_results/test_summary.json)\n",
    "v1 = {\n",
    "    'ridge':             {'rmse': 72.73, 'mae': 61.29, 'phm08_score': 786263, 'bias': 42.5},\n",
    "    'gradient_boosting': {'rmse': 52.69, 'mae': 45.00, 'phm08_score': 54255,  'bias': 37.1},\n",
    "    'random_forest':     {'rmse': 51.49, 'mae': 43.88, 'phm08_score': 45151,  'bias': 35.1},\n",
    "}\n",
    "\n",
    "print(f'{\"Model\":>20s}  {\"v1 RMSE\":>8s}  {\"v2 RMSE\":>8s}  {\"delta\":>8s}  {\"v1 PHM08\":>10s}  {\"v2 PHM08\":>10s}')\n",
    "print('-' * 70)\n",
    "for name in ['ridge', 'random_forest', 'gradient_boosting']:\n",
    "    v1r = v1[name]['rmse']\n",
    "    v2r = test_results[name]['rmse']\n",
    "    v1p = v1[name]['phm08_score']\n",
    "    v2p = test_results[name]['phm08_score']\n",
    "    delta = v2r - v1r\n",
    "    arrow = 'better' if delta < 0 else 'worse'\n",
    "    print(f'{name:>20s}  {v1r:>8.2f}  {v2r:>8.2f}  {delta:>+8.2f}  {v1p:>10.0f}  {v2p:>10.0f}  {arrow}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = TRAIN_DIR / 'ml_results_v2'\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "summary = {\n",
    "    'experiment': EXPERIMENT,\n",
    "    'dataset': 'FD001',\n",
    "    'evaluation': 'official_test_split',\n",
    "    'change': 'causal backward derivatives (no future lookahead)',\n",
    "    'n_train_rows': int(X_train.shape[0]),\n",
    "    'n_train_features': int(X_train.shape[1]),\n",
    "    'n_test_engines': len(test_cohorts),\n",
    "    'rul_cap': RUL_CAP,\n",
    "    'best_model': min(test_results, key=lambda k: test_results[k]['rmse']),\n",
    "    'cv_results': cv_results,\n",
    "    'test_results': test_results,\n",
    "}\n",
    "\n",
    "with open(out_dir / 'model_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f'Saved to {out_dir}')\n",
    "print(json.dumps(summary, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}