{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FD001 RUL Prediction — Experiment v2\n",
    "\n",
    "**Change from v1:** `compute_derivatives()` in `packages/geometry/src/geometry/dynamics.py` was non-causal.\n",
    "Central differences + symmetric smoothing meant `effective_dim_velocity[t]` used windows `t+1` and `t+2`.\n",
    "\n",
    "**v2 fix:**\n",
    "- Backward-only smoothing (cumsum-based causal moving average)\n",
    "- Backward finite differences (velocity at t uses only t and t-1)\n",
    "- Also fixed `smooth_window` passthrough bug: orchestration pipeline now reads `derivative_depth` from manifest\n",
    "\n",
    "**Parquets regenerated:** FD001 Train (220s) and Test (153s) re-run with causal derivatives.\n",
    "\n",
    "**Hypothesis:** Geodyn features (effective_dim_velocity, spectral_gap_velocity, etc.) should transfer better\n",
    "across cohorts now that they don't encode future information. Centroid features (cv_*) should be unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:02.366912Z",
     "iopub.status.busy": "2026-02-25T18:21:02.366682Z",
     "iopub.status.idle": "2026-02-25T18:21:03.169167Z",
     "shell.execute_reply": "2026-02-25T18:21:03.168600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:03.170374Z",
     "iopub.status.busy": "2026-02-25T18:21:03.170260Z",
     "iopub.status.idle": "2026-02-25T18:21:03.172092Z",
     "shell.execute_reply": "2026-02-25T18:21:03.171791Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = Path('/Users/jasonrudder/domains/cmapss/FD_001/Train/output_time')\n",
    "TEST_DIR  = Path('/Users/jasonrudder/domains/cmapss/FD_001/Test/output_time')\n",
    "RUL_PATH  = Path('/Users/jasonrudder/domains/cmapss/FD_001/RUL_FD001.txt')\n",
    "OBS_TRAIN = TRAIN_DIR / 'observations.parquet'\n",
    "OBS_TEST  = TEST_DIR / 'observations.parquet'\n",
    "\n",
    "EXPERIMENT = 'v2_causal_derivatives'\n",
    "RUL_CAP = 125\n",
    "N_FOLDS = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:03.173042Z",
     "iopub.status.busy": "2026-02-25T18:21:03.172983Z",
     "iopub.status.idle": "2026-02-25T18:21:03.177990Z",
     "shell.execute_reply": "2026-02-25T18:21:03.177602Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_features(output_dir: Path, obs_path: Path) -> pl.DataFrame:\n",
    "    \"\"\"Build ML feature matrix from pipeline parquets.\"\"\"\n",
    "    # Cohort vector (base)\n",
    "    cv = pl.read_parquet(str(output_dir / 'cohort/cohort_vector.parquet'))\n",
    "    join_cols = ['cohort', 'signal_0_center']\n",
    "    feat_cols = [c for c in cv.columns if c not in ['signal_0_start', 'signal_0_end', 'signal_0_center', 'cohort', 'n_signals']]\n",
    "    ml = cv.select(['cohort', 'signal_0_center'] + feat_cols).rename({c: f'cv_{c}' for c in feat_cols})\n",
    "    print(f'  cohort_vector: {len(feat_cols)} features, {len(ml)} rows')\n",
    "\n",
    "    # Geometry dynamics\n",
    "    gd_path = output_dir / 'cohort/cohort_dynamics/geometry_dynamics.parquet'\n",
    "    if gd_path.exists():\n",
    "        gd = pl.read_parquet(str(gd_path))\n",
    "        gd_feat = [c for c in gd.columns if c not in ['cohort', 'signal_0_start', 'signal_0_end', 'signal_0_center', 'I']]\n",
    "        gd_sel = gd.select(['cohort', 'signal_0_center'] + gd_feat).rename({c: f'gd_{c}' for c in gd_feat})\n",
    "        before = ml.shape[1]\n",
    "        ml = ml.join(gd_sel, on=join_cols, how='left', coalesce=True)\n",
    "        print(f'  + geometry_dynamics: {ml.shape[1] - before} features')\n",
    "\n",
    "    # Velocity field (aggregate per cohort+window since it's per-signal)\n",
    "    vf_path = output_dir / 'cohort/cohort_dynamics/velocity_field.parquet'\n",
    "    if vf_path.exists():\n",
    "        vf = pl.read_parquet(str(vf_path))\n",
    "        num_cols = [c for c in vf.columns if c not in ['cohort', 'signal_0_start', 'signal_0_end', 'signal_0_center',\n",
    "                    'signal_id', 'dominant_motion_signal'] and vf[c].dtype in [pl.Float64, pl.Float32, pl.Int64]]\n",
    "        if 'signal_0_center' in vf.columns and num_cols:\n",
    "            vf_agg = vf.group_by(['cohort', 'signal_0_center']).agg(\n",
    "                [pl.col(c).mean().alias(f'vf_{c}_mean') for c in num_cols] +\n",
    "                [pl.col(c).std().alias(f'vf_{c}_std') for c in num_cols]\n",
    "            )\n",
    "            before = ml.shape[1]\n",
    "            ml = ml.join(vf_agg, on=join_cols, how='left', coalesce=True)\n",
    "            print(f'  + velocity_field: {ml.shape[1] - before} features')\n",
    "\n",
    "    # Cohort geometry\n",
    "    cg_path = output_dir / 'cohort/cohort_geometry.parquet'\n",
    "    if cg_path.exists():\n",
    "        cg = pl.read_parquet(str(cg_path))\n",
    "        cg_feat = [c for c in cg.columns if c not in ['cohort', 'signal_0_start', 'signal_0_end', 'signal_0_center',\n",
    "                    'window_index', 'signal_id'] and cg[c].dtype in [pl.Float64, pl.Float32]]\n",
    "        if 'signal_0_center' in cg.columns and cg_feat:\n",
    "            cg_agg = cg.group_by(['cohort', 'signal_0_center']).agg(\n",
    "                [pl.col(c).mean().alias(f'cg_{c}_mean') for c in cg_feat]\n",
    "            )\n",
    "            before = ml.shape[1]\n",
    "            ml = ml.join(cg_agg, on=join_cols, how='left', coalesce=True)\n",
    "            print(f'  + cohort_geometry: {ml.shape[1] - before} features')\n",
    "\n",
    "    # Add RUL\n",
    "    obs = pl.read_parquet(str(obs_path))\n",
    "    first_sig = obs['signal_id'].unique().sort()[0]\n",
    "    lifecycles = dict(\n",
    "        obs.filter(pl.col('signal_id') == first_sig)\n",
    "        .group_by('cohort').agg(pl.col('signal_0').max().alias('max_s0'))\n",
    "        .iter_rows()\n",
    "    )\n",
    "\n",
    "    rul_rows = []\n",
    "    for row in ml.select(['cohort', 'signal_0_center']).iter_rows(named=True):\n",
    "        max_s0 = lifecycles.get(row['cohort'])\n",
    "        if max_s0 is not None:\n",
    "            rul_rows.append({\n",
    "                'cohort': row['cohort'],\n",
    "                'signal_0_center': row['signal_0_center'],\n",
    "                'RUL': max_s0 - row['signal_0_center'],\n",
    "                'lifecycle': max_s0,\n",
    "                'lifecycle_pct': row['signal_0_center'] / max_s0 if max_s0 > 0 else 0,\n",
    "            })\n",
    "\n",
    "    rul_df = pl.DataFrame(rul_rows)\n",
    "    ml = ml.join(rul_df, on=['cohort', 'signal_0_center'], how='left', coalesce=True)\n",
    "\n",
    "    # Drop constant columns\n",
    "    feat_cols = [c for c in ml.columns if c not in ['cohort', 'signal_0_center', 'RUL', 'lifecycle', 'lifecycle_pct']]\n",
    "    drop_const = [c for c in feat_cols if ml[c].dtype in [pl.Float64, pl.Float32] and\n",
    "                  (ml[c].drop_nulls().std() or 0) < 1e-10]\n",
    "    if drop_const:\n",
    "        ml = ml.drop(drop_const)\n",
    "        print(f'  Dropped {len(drop_const)} constant columns')\n",
    "\n",
    "    feat_final = [c for c in ml.columns if c not in ['cohort', 'signal_0_center', 'RUL', 'lifecycle', 'lifecycle_pct']]\n",
    "    print(f'  Total: {len(feat_final)} features, {len(ml)} rows, {ml[\"cohort\"].n_unique()} cohorts')\n",
    "    return ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:03.178839Z",
     "iopub.status.busy": "2026-02-25T18:21:03.178786Z",
     "iopub.status.idle": "2026-02-25T18:21:03.221652Z",
     "shell.execute_reply": "2026-02-25T18:21:03.221193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN ===\n",
      "  cohort_vector: 155 features, 789 rows\n",
      "  + geometry_dynamics: 72 features\n",
      "  + velocity_field: 14 features\n",
      "  + cohort_geometry: 39 features\n",
      "  Dropped 13 constant columns\n",
      "  Total: 267 features, 789 rows, 100 cohorts\n",
      "\n",
      "=== TEST ===\n",
      "  cohort_vector: 151 features, 675 rows\n",
      "  + geometry_dynamics: 72 features\n",
      "  + velocity_field: 14 features\n",
      "  + cohort_geometry: 39 features\n",
      "  Dropped 9 constant columns\n",
      "  Total: 267 features, 675 rows, 99 cohorts\n"
     ]
    }
   ],
   "source": [
    "print('=== TRAIN ===')\n",
    "ml_train = build_features(TRAIN_DIR, OBS_TRAIN)\n",
    "print(f'\\n=== TEST ===')\n",
    "ml_test = build_features(TEST_DIR, OBS_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Validation on Train (GroupKFold by cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:03.235454Z",
     "iopub.status.busy": "2026-02-25T18:21:03.235360Z",
     "iopub.status.idle": "2026-02-25T18:21:03.247950Z",
     "shell.execute_reply": "2026-02-25T18:21:03.247560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 789 rows x 267 features\n",
      "RUL range (capped): 22 – 125\n",
      "Any inf remaining: False\n",
      "Any NaN remaining: False\n"
     ]
    }
   ],
   "source": [
    "META = ['cohort', 'signal_0_center', 'RUL', 'lifecycle', 'lifecycle_pct']\n",
    "\n",
    "def prepare(ml, cap=RUL_CAP):\n",
    "    feat_cols = [c for c in ml.columns if c not in META]\n",
    "    X = ml.select(feat_cols).to_numpy().astype(np.float64)\n",
    "    y = np.minimum(ml['RUL'].to_numpy().astype(np.float64), cap)\n",
    "    groups = ml['cohort'].to_numpy()\n",
    "    # Replace inf BEFORE imputation (SimpleImputer chokes on inf)\n",
    "    X = np.where(np.isinf(X), np.nan, X)\n",
    "    imp = SimpleImputer(strategy='median')\n",
    "    X = imp.fit_transform(X)\n",
    "    # Belt-and-suspenders: catch any remaining non-finite\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X, y, groups, feat_cols, imp\n",
    "\n",
    "X_train, y_train, groups_train, feat_names, imputer = prepare(ml_train)\n",
    "print(f'Train: {X_train.shape[0]} rows x {X_train.shape[1]} features')\n",
    "print(f'RUL range (capped): {y_train.min():.0f} – {y_train.max():.0f}')\n",
    "print(f'Any inf remaining: {np.any(np.isinf(X_train))}')\n",
    "print(f'Any NaN remaining: {np.any(np.isnan(X_train))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:03.248962Z",
     "iopub.status.busy": "2026-02-25T18:21:03.248884Z",
     "iopub.status.idle": "2026-02-25T18:21:27.801239Z",
     "shell.execute_reply": "2026-02-25T18:21:27.800812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ridge  RMSE=9.35 ± 0.89  MAE=6.81  R²=0.9251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       random_forest  RMSE=9.11 ± 0.82  MAE=6.41  R²=0.9291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gradient_boosting  RMSE=8.56 ± 1.09  MAE=5.80  R²=0.9368\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'ridge': Ridge(alpha=1.0),\n",
    "    'random_forest': RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=10, min_samples_leaf=5,\n",
    "        random_state=SEED, n_jobs=-1),\n",
    "    'gradient_boosting': GradientBoostingRegressor(\n",
    "        n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "        min_samples_leaf=5, subsample=0.8, random_state=SEED),\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "\n",
    "for name, model in models.items():\n",
    "    rmses, maes, r2s = [], [], []\n",
    "    for fold, (tr_idx, te_idx) in enumerate(gkf.split(X_train, y_train, groups_train)):\n",
    "        scaler = StandardScaler()\n",
    "        Xtr = scaler.fit_transform(X_train[tr_idx])\n",
    "        Xte = scaler.transform(X_train[te_idx])\n",
    "        m = model.__class__(**model.get_params())\n",
    "        m.fit(Xtr, y_train[tr_idx])\n",
    "        pred = np.clip(m.predict(Xte), 0, RUL_CAP)\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_train[te_idx], pred)))\n",
    "        maes.append(mean_absolute_error(y_train[te_idx], pred))\n",
    "        r2s.append(r2_score(y_train[te_idx], pred))\n",
    "    cv_results[name] = {\n",
    "        'rmse': np.mean(rmses), 'rmse_std': np.std(rmses),\n",
    "        'mae': np.mean(maes), 'mae_std': np.std(maes),\n",
    "        'r2': np.mean(r2s),\n",
    "    }\n",
    "    print(f'{name:>20s}  RMSE={np.mean(rmses):.2f} ± {np.std(rmses):.2f}  '\n",
    "          f'MAE={np.mean(maes):.2f}  R²={np.mean(r2s):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Official Test Evaluation (Train→Test transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:27.802612Z",
     "iopub.status.busy": "2026-02-25T18:21:27.802546Z",
     "iopub.status.idle": "2026-02-25T18:21:27.806097Z",
     "shell.execute_reply": "2026-02-25T18:21:27.805797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth RUL: 100 engines\n",
      "Common features: 267 / 267\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth RUL for test engines\n",
    "rul_gt = np.loadtxt(str(RUL_PATH))\n",
    "rul_gt_capped = np.clip(rul_gt, 0, RUL_CAP)\n",
    "print(f'Ground truth RUL: {len(rul_gt)} engines')\n",
    "\n",
    "# Align test features: use common columns with train\n",
    "test_feat_cols = [c for c in ml_test.columns if c not in META]\n",
    "common_feats = sorted(set(feat_names) & set(test_feat_cols))\n",
    "missing_in_test = sorted(set(feat_names) - set(test_feat_cols))\n",
    "print(f'Common features: {len(common_feats)} / {len(feat_names)}')\n",
    "if missing_in_test:\n",
    "    print(f'Missing in test: {len(missing_in_test)} — will fill with 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:27.807237Z",
     "iopub.status.busy": "2026-02-25T18:21:27.807186Z",
     "iopub.status.idle": "2026-02-25T18:21:27.883600Z",
     "shell.execute_reply": "2026-02-25T18:21:27.883209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test engines: 99\n",
      "Ground truth aligned: 99 engines\n"
     ]
    }
   ],
   "source": [
    "# Build test matrix aligned with train columns\n",
    "X_test_full = np.zeros((len(ml_test), len(feat_names)))\n",
    "for i, col in enumerate(feat_names):\n",
    "    if col in test_feat_cols:\n",
    "        X_test_full[:, i] = ml_test[col].to_numpy().astype(np.float64)\n",
    "\n",
    "# Replace inf before imputer, then impute, then catch stragglers\n",
    "X_test_full = np.where(np.isinf(X_test_full), np.nan, X_test_full)\n",
    "X_test_full = imputer.transform(X_test_full)\n",
    "X_test_full = np.nan_to_num(X_test_full, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Get last window per test cohort (closest to failure)\n",
    "test_cohorts = ml_test['cohort'].unique().sort().to_list()\n",
    "last_idx = []\n",
    "for coh in test_cohorts:\n",
    "    mask = ml_test['cohort'] == coh\n",
    "    coh_rows = ml_test.filter(mask)\n",
    "    max_s0 = coh_rows['signal_0_center'].max()\n",
    "    idx = ml_test.with_row_index('_idx').filter(\n",
    "        (pl.col('cohort') == coh) & (pl.col('signal_0_center') == max_s0)\n",
    "    )['_idx'][0]\n",
    "    last_idx.append(idx)\n",
    "\n",
    "X_test_last = X_test_full[last_idx]\n",
    "print(f'Test engines: {len(X_test_last)}')\n",
    "\n",
    "# Map test cohorts to RUL ground truth (engine_1 → index 0, etc.)\n",
    "cohort_to_idx = {}\n",
    "for coh in test_cohorts:\n",
    "    num = int(coh.replace('engine_', ''))\n",
    "    cohort_to_idx[coh] = num - 1  # 1-indexed to 0-indexed\n",
    "\n",
    "y_test_gt = np.array([rul_gt_capped[cohort_to_idx[c]] for c in test_cohorts])\n",
    "print(f'Ground truth aligned: {len(y_test_gt)} engines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:27.884630Z",
     "iopub.status.busy": "2026-02-25T18:21:27.884559Z",
     "iopub.status.idle": "2026-02-25T18:21:34.263709Z",
     "shell.execute_reply": "2026-02-25T18:21:34.263385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ridge  RMSE=25.75  MAE=21.66  R²=0.5877  PHM08=1934  bias=19.5 (13E/83L)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       random_forest  RMSE=26.04  MAE=21.82  R²=0.5781  PHM08=2378  bias=19.6 (17E/82L)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gradient_boosting  RMSE=25.66  MAE=21.54  R²=0.5903  PHM08=2243  bias=19.6 (15E/83L)\n"
     ]
    }
   ],
   "source": [
    "# Train on full train set, predict on test\n",
    "scaler_full = StandardScaler()\n",
    "X_train_scaled = scaler_full.fit_transform(X_train)\n",
    "X_test_scaled = scaler_full.transform(X_test_last)\n",
    "\n",
    "test_results = {}\n",
    "for name, model in models.items():\n",
    "    m = model.__class__(**model.get_params())\n",
    "    m.fit(X_train_scaled, y_train)\n",
    "    pred = np.clip(m.predict(X_test_scaled), 0, RUL_CAP)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_gt, pred))\n",
    "    mae = mean_absolute_error(y_test_gt, pred)\n",
    "    r2 = r2_score(y_test_gt, pred)\n",
    "    \n",
    "    # PHM08 scoring function\n",
    "    d = pred - y_test_gt\n",
    "    phm = np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1))\n",
    "    bias = np.mean(d)\n",
    "    n_early = np.sum(d < 0)\n",
    "    n_late = np.sum(d > 0)\n",
    "    \n",
    "    test_results[name] = {\n",
    "        'rmse': rmse, 'mae': mae, 'r2': r2,\n",
    "        'phm08_score': phm, 'bias': bias,\n",
    "        'n_early': int(n_early), 'n_late': int(n_late),\n",
    "    }\n",
    "    print(f'{name:>20s}  RMSE={rmse:.2f}  MAE={mae:.2f}  R²={r2:.4f}  '\n",
    "          f'PHM08={phm:.0f}  bias={bias:.1f} ({n_early}E/{n_late}L)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:34.264900Z",
     "iopub.status.busy": "2026-02-25T18:21:34.264836Z",
     "iopub.status.idle": "2026-02-25T18:21:39.818394Z",
     "shell.execute_reply": "2026-02-25T18:21:39.817935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank                                             Feature  Importance  Group\n",
      "---------------------------------------------------------------------------\n",
      "   1                          cv_centroid_adf_stat_value      0.4885     cv\n",
      "   2                 cv_centroid_frequency_bands_low_rel      0.2651     cv\n",
      "   3                                cv_centroid_trend_r2      0.0970     cv\n",
      "   4                         cv_centroid_adf_stat_pvalue      0.0244     cv\n",
      "   5                         cv_centroid_thd_n_harmonics      0.0213     cv\n",
      "   6                  cv_centroid_fundamental_freq_ratio      0.0082     cv\n",
      "   7                 cv_centroid_frequency_bands_mid_rel      0.0070     cv\n",
      "   8                   cv_centroid_recurrence_rate_value      0.0058     cv\n",
      "   9             cv_centroid_frequency_bands_total_power      0.0040     cv\n",
      "  10                     cv_centroid_rqa_recurrence_rate      0.0036     cv\n",
      "  11                    cv_centroid_frequency_bands_high      0.0036     cv\n",
      "  12              cv_centroid_harmonics_fundamental_freq      0.0030     cv\n",
      "  13                       cv_centroid_spectral_centroid      0.0024     cv\n",
      "  14                        cv_centroid_spectral_entropy      0.0024     cv\n",
      "  15                       cv_centroid_lyapunov_exponent      0.0024     cv\n",
      "  16                   cv_centroid_basin_transition_prob      0.0020     cv\n",
      "  17                          cv_centroid_spectral_slope      0.0019     cv\n",
      "  18                           gd_eigenvalue_0_curvature      0.0018     gd\n",
      "  19                  cv_centroid_fundamental_freq_value      0.0016     cv\n",
      "  20          cv_centroid_complexity_permutation_entropy      0.0014     cv\n",
      "  21                               gd_effective_dim_jerk      0.0012     gd\n",
      "  22             cv_centroid_fundamental_freq_confidence      0.0011     cv\n",
      "  23                          gd_effective_dim_curvature      0.0010     gd\n",
      "  24               cv_centroid_attractor_correlation_dim      0.0010     cv\n",
      "  25                             cv_centroid_thd_percent      0.0010     cv\n"
     ]
    }
   ],
   "source": [
    "# Train GB on full data for importance\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "    min_samples_leaf=5, subsample=0.8, random_state=SEED)\n",
    "gb.fit(X_train_scaled, y_train)\n",
    "\n",
    "imp_idx = np.argsort(gb.feature_importances_)[::-1]\n",
    "print(f'{\"Rank\":>4s}  {\"Feature\":>50s}  {\"Importance\":>10s}  {\"Group\":>5s}')\n",
    "print('-' * 75)\n",
    "for rank, i in enumerate(imp_idx[:25]):\n",
    "    feat = feat_names[i]\n",
    "    group = feat.split('_')[0]\n",
    "    print(f'{rank+1:>4d}  {feat:>50s}  {gb.feature_importances_[i]:>10.4f}  {group:>5s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison with v1 (pre-causal fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:39.819519Z",
     "iopub.status.busy": "2026-02-25T18:21:39.819448Z",
     "iopub.status.idle": "2026-02-25T18:21:39.821863Z",
     "shell.execute_reply": "2026-02-25T18:21:39.821518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model   v1 RMSE   v2 RMSE     delta    v1 PHM08    v2 PHM08\n",
      "----------------------------------------------------------------------\n",
      "               ridge     72.73     25.75    -46.98      786263        1934  better\n",
      "       random_forest     51.49     26.04    -25.45       45151        2378  better\n",
      "   gradient_boosting     52.69     25.66    -27.03       54255        2243  better\n"
     ]
    }
   ],
   "source": [
    "# v1 results (from ~/domains/testing/FD_001/test/output_sequential/ml_results/test_summary.json)\n",
    "v1 = {\n",
    "    'ridge':             {'rmse': 72.73, 'mae': 61.29, 'phm08_score': 786263, 'bias': 42.5},\n",
    "    'gradient_boosting': {'rmse': 52.69, 'mae': 45.00, 'phm08_score': 54255,  'bias': 37.1},\n",
    "    'random_forest':     {'rmse': 51.49, 'mae': 43.88, 'phm08_score': 45151,  'bias': 35.1},\n",
    "}\n",
    "\n",
    "print(f'{\"Model\":>20s}  {\"v1 RMSE\":>8s}  {\"v2 RMSE\":>8s}  {\"delta\":>8s}  {\"v1 PHM08\":>10s}  {\"v2 PHM08\":>10s}')\n",
    "print('-' * 70)\n",
    "for name in ['ridge', 'random_forest', 'gradient_boosting']:\n",
    "    v1r = v1[name]['rmse']\n",
    "    v2r = test_results[name]['rmse']\n",
    "    v1p = v1[name]['phm08_score']\n",
    "    v2p = test_results[name]['phm08_score']\n",
    "    delta = v2r - v1r\n",
    "    arrow = 'better' if delta < 0 else 'worse'\n",
    "    print(f'{name:>20s}  {v1r:>8.2f}  {v2r:>8.2f}  {delta:>+8.2f}  {v1p:>10.0f}  {v2p:>10.0f}  {arrow}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T18:21:39.822831Z",
     "iopub.status.busy": "2026-02-25T18:21:39.822778Z",
     "iopub.status.idle": "2026-02-25T18:21:39.825357Z",
     "shell.execute_reply": "2026-02-25T18:21:39.825030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /Users/jasonrudder/domains/cmapss/FD_001/Train/output_time/ml_results_v2\n",
      "{\n",
      "  \"experiment\": \"v2_causal_derivatives\",\n",
      "  \"dataset\": \"FD001\",\n",
      "  \"evaluation\": \"official_test_split\",\n",
      "  \"change\": \"causal backward derivatives (no future lookahead)\",\n",
      "  \"n_train_rows\": 789,\n",
      "  \"n_train_features\": 267,\n",
      "  \"n_test_engines\": 99,\n",
      "  \"rul_cap\": 125,\n",
      "  \"best_model\": \"gradient_boosting\",\n",
      "  \"cv_results\": {\n",
      "    \"ridge\": {\n",
      "      \"rmse\": 9.350570249890687,\n",
      "      \"rmse_std\": 0.8914680534917707,\n",
      "      \"mae\": 6.807672679939131,\n",
      "      \"mae_std\": 0.746367865454528,\n",
      "      \"r2\": 0.9251225343760551\n",
      "    },\n",
      "    \"random_forest\": {\n",
      "      \"rmse\": 9.11254294278877,\n",
      "      \"rmse_std\": 0.8195732731314369,\n",
      "      \"mae\": 6.41060899564777,\n",
      "      \"mae_std\": 0.5339540634029992,\n",
      "      \"r2\": 0.929074780772727\n",
      "    },\n",
      "    \"gradient_boosting\": {\n",
      "      \"rmse\": 8.561714905847818,\n",
      "      \"rmse_std\": 1.0919488600985428,\n",
      "      \"mae\": 5.804011316281228,\n",
      "      \"mae_std\": 0.7497683297739648,\n",
      "      \"r2\": 0.9368136740904749\n",
      "    }\n",
      "  },\n",
      "  \"test_results\": {\n",
      "    \"ridge\": {\n",
      "      \"rmse\": 25.746523675775663,\n",
      "      \"mae\": 21.65983275849084,\n",
      "      \"r2\": 0.5876826352003175,\n",
      "      \"phm08_score\": 1933.5430882976236,\n",
      "      \"bias\": 19.53603494585641,\n",
      "      \"n_early\": 13,\n",
      "      \"n_late\": 83\n",
      "    },\n",
      "    \"random_forest\": {\n",
      "      \"rmse\": 26.043646023331565,\n",
      "      \"mae\": 21.824693391691135,\n",
      "      \"r2\": 0.578111199623534,\n",
      "      \"phm08_score\": 2378.358131605592,\n",
      "      \"bias\": 19.580903325905638,\n",
      "      \"n_early\": 17,\n",
      "      \"n_late\": 82\n",
      "    },\n",
      "    \"gradient_boosting\": {\n",
      "      \"rmse\": 25.664082216648012,\n",
      "      \"mae\": 21.5447546880896,\n",
      "      \"r2\": 0.590318923005308,\n",
      "      \"phm08_score\": 2243.3990607995793,\n",
      "      \"bias\": 19.56561730634849,\n",
      "      \"n_early\": 15,\n",
      "      \"n_late\": 83\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "out_dir = TRAIN_DIR / 'ml_results_v2'\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "summary = {\n",
    "    'experiment': EXPERIMENT,\n",
    "    'dataset': 'FD001',\n",
    "    'evaluation': 'official_test_split',\n",
    "    'change': 'causal backward derivatives (no future lookahead)',\n",
    "    'n_train_rows': int(X_train.shape[0]),\n",
    "    'n_train_features': int(X_train.shape[1]),\n",
    "    'n_test_engines': len(test_cohorts),\n",
    "    'rul_cap': RUL_CAP,\n",
    "    'best_model': min(test_results, key=lambda k: test_results[k]['rmse']),\n",
    "    'cv_results': cv_results,\n",
    "    'test_results': test_results,\n",
    "}\n",
    "\n",
    "with open(out_dir / 'model_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f'Saved to {out_dir}')\n",
    "print(json.dumps(summary, indent=2, default=str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
